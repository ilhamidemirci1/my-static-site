{
    "articles": [
        {
            "id": "https___azure_microsoft_com_en-us_blog__p_39302",
            "title": "Accelerating agentic workflows with Azure AI Foundry, NVIDIA NIM, and NVIDIA AgentIQ",
            "source": "https://azure.microsoft.com/en-us/blog/accelerating-agentic-workflows-with-azure-ai-foundry-nvidia-nim-and-nvidia-agentiq/",
            "content": "I’m excited to share a major leap forward in how we develop and deploy AI. In collaboration with NVIDIA, we’ve integrated NVIDIA NIM microservices and NVIDIA AgentIQ toolkit into Azure AI Foundry—unlocking unprecedented efficiency, performance, and cost optimization for your AI projects.     Create with Azure AI Foundry >  A new era of AI efficiency  In today’s fast-paced digital landscape, scaling AI applications demands more than just innovation—it requires streamlined processes that deliver rapid time-to-market without compromising on performance. With enterprise AI projects often taking 9 to 12 months to move from conception to production, every efficiency gain counts. Our integration is designed to change that by simplifying every step of the AI development lifecycle.  NVIDIA NIM on Azure AI Foundry  NVIDIA NIM™, part of the NVIDIA AI Enterprise software suite, is a suite of easy-to-use microservices engineered for secure, reliable, and high-performance AI inferencing. Leveraging robust technologies such as NVIDIA Triton Inference Server™, TensorRT™, TensorRT-LLM, and PyTorch, NIM microservices are built to scale seamlessly on managed Azure compute.  They provide:   Zero-configuration deployment: Get up and running quickly with out-of-the-box optimization.    Seamless Azure integration: Works effortlessly with Azure AI Agent Service and Semantic Kernel.    Enterprise-grade reliability: Benefit from NVIDIA AI Enterprise support for continuous performance and security.    Scalable inference: Tap into Azure’s NVIDIA accelerated infrastructure for demanding workloads.    Optimized workflows: Accelerate applications ranging from large language models to advanced analytics.    Deploying these services is simple. With just a few clicks—whether selecting models like the Llama-3.3-70B-NIM or others from the model catalog in Azure AI Foundry—you can integrate them directly into your AI workflows and start building generative AI applications that work flawlessly within the Azure ecosystem.  Optimizing performance with NVIDIA AgentIQ  Once your NVIDIA NIM microservices are deployed, NVIDIA AgentIQ takes center stage. This open-source toolkit is designed to seamlessly connect, profile, and optimize teams of AI agents, enables your systems to run at peak performance. AgentIQ delivers:   Profiling and optimization: Leverage real-time telemetry to fine-tune AI agent placement, reducing latency and compute overhead.    Dynamic inference enhancements: Continuously collect and analyze metadata—such as predicted output tokens per call, estimated time to next inference, and expected token lengths—to dynamically improve agent performance.    Integration with Semantic Kernel: Direct integration with Azure AI Foundry Agent Service further empowers your agents with enhanced semantic reasoning and task execution capabilities.    This intelligent profiling not only reduces compute costs but also boosts accuracy and responsiveness, so that every part of your agentic AI workflow is optimized for success.  In addition, we will soon be integrating the NVIDIA Llama Nemotron Reason open reasoning model. NVIDIA Llama Nemotron Reason is a powerful AI model family designed for advanced reasoning. According to NVIDIA, Nemotron excels at coding, complex math, and scientific reasoning while understanding user intent and seamlessly calling tools like search and translations to accomplish tasks. Real-world impact  Industry leaders are already witnessing the benefits of these innovations.  Drew McCombs, Vice President, Cloud and Analytics at Epic, noted:   The launch of NVIDIA NIM microservices in Azure AI Foundry offers a secure and efficient way for Epic to deploy open-source generative AI models that improve patient care, boost clinician and operational efficiency, and uncover new insights to drive medical innovation. In collaboration with UW Health and UC San Diego Health, we’re also researching methods to evaluate clinical summaries with these advanced models. Together, we’re using the latest AI technology in ways that truly improve the lives of clinicians and patients.  Epic’s experience underscores how our integrated solution can drive transformational change—not just in healthcare but across every industry where high-performance AI is a game changer. As noted by Jon Sigler, EVP, Platform and AI at ServiceNow:  This combination of ServiceNow’s AI platform with NVIDIA NIM and Microsoft Azure AI Foundry and Azure AI Agent Service helps us bring to market industry-specific, out-of-the-box AI agents, delivering full-stack agentic AI solutions to help resolve problems faster, deliver great customer experiences, and accelerate improvements in organizations’ productivity and efficiency.  Unlock AI-powered innovation  By combining the robust deployment capabilities of NVIDIA NIM with the dynamic optimization of NVIDIA AgentIQ, Azure AI Foundry provides a turnkey solution for building, deploying, and scaling enterprise-grade agentic applications. This integration can accelerate AI deployments, enhance agentic workflows, and reduce infrastructure costs—enabling you to focus on what truly matters: driving innovation.  Ready to accelerate your AI journey?  Deploy NVIDIA NIM microservices and optimize your AI agents with NVIDIA AgentIQ toolkit on Azure AI Foundry. Explore more about the Azure AI Foundry model catalog. Let’s build a smarter, faster, and more efficient future together.        Azure AI Foundry Design, customize, and manage AI apps and agents at scale.         Discover more >           The post Accelerating agentic workflows with Azure AI Foundry, NVIDIA NIM, and NVIDIA AgentIQ appeared first on Microsoft Azure Blog.",
            "url": "https://azure.microsoft.com/en-us/blog/accelerating-agentic-workflows-with-azure-ai-foundry-nvidia-nim-and-nvidia-agentiq/",
            "published_date": "2025-03-18T20:00:00Z",
            "processed": true,
            "phrase_refs": {
                "Azure AI Foundry": "https://azure.microsoft.com/en-us/products/ai-foundry",
                "Create with Azure AI Foundry >": "https://azure.microsoft.com/en-us/products/ai-foundry",
                "open-source toolkit": "https://github.com/microsoft/semantic-kernel/tree/main/python/semantic_kernel/connectors/ai/nvidia",
                "optimize your AI agents": "https://github.com/microsoft/semantic-kernel/tree/main/python/semantic_kernel/connectors/ai/nvidia",
                "model catalog": "https://ai.azure.com/explore/models?tid=72f988bf-86f1-41af-91ab-2d7cd011db47",
                "Discover more >": "https://azure.microsoft.com/en-us/products/ai-foundry",
                "Accelerating agentic workflows with Azure AI Foundry, NVIDIA NIM, and NVIDIA AgentIQ": "https://azure.microsoft.com/en-us/blog/accelerating-agentic-workflows-with-azure-ai-foundry-nvidia-nim-and-nvidia-agentiq/",
                "Microsoft Azure Blog": "https://azure.microsoft.com/en-us/blog"
            },
            "_rid": "JfZBAIxX5c4BAAAAAAAAAA==",
            "_self": "dbs/JfZBAA==/colls/JfZBAIxX5c4=/docs/JfZBAIxX5c4BAAAAAAAAAA==/",
            "_etag": "\"00007ed3-0000-1500-0000-67e537660000\"",
            "_attachments": "attachments/",
            "ai_summary": "Microsoft has partnered with NVIDIA to enhance AI development and deployment through the integration of NVIDIA NIM microservices and the NVIDIA AgentIQ toolkit into Azure AI Foundry. This collaboration aims to streamline the AI development lifecycle, significantly reducing the time required to bring enterprise AI projects to production while optimizing performance and costs. By leveraging NVIDIA's technologies, users can deploy scalable AI applications with improved efficiency, enabling transformative impacts across various industries.",
            "process_date": "2025-03-27T14:32:54.215139",
            "published": false,
            "publication_date": null,
            "_ts": 1743075174
        },
        {
            "id": "https___azure_microsoft_com_en-us_blog__p_39235",
            "title": "Microsoft and NVIDIA accelerate AI development and performance",
            "source": "https://azure.microsoft.com/en-us/blog/microsoft-and-nvidia-accelerate-ai-development-and-performance/",
            "content": "Together, Microsoft and NVIDIA are accelerating some of the most groundbreaking innovations in AI. This long-standing collaboration has been at the core of the AI revolution over the past few years, from bringing industry-leading supercomputing performance in the cloud to supporting breakthrough frontier models and solutions like ChatGPT in Microsoft Azure OpenAI Service and Microsoft Copilot. Today, there are several new announcements from Microsoft and NVIDIA that further enhance the full stack collaboration to help shape the future of AI. This includes integrating the newest NVIDIA Blackwell platform with Azure AI services infrastructure, incorporating NVIDIA NIM microservices into Azure AI Foundry, and empowering developers, startups, and organizations of all sizes like NBA, BMW, Dentsu, Harvey and OriGen, to accelerate their innovations and solve the most challenging problems across domains.  Come see Microsoft at the NVIDIA GTC AI Conference  Empowering all developers and innovators with agentic AI  Microsoft and NVIDIA collaborate deeply across the entire technology stack, and with the rise of agentic AI, they are thrilled to share several new offerings that are available in Azure AI Foundry. First is that Azure AI Foundry now offers NVIDIA NIM microservices. NIM provides optimized containers for more than two dozen popular foundation models, allowing developers to deploy generative AI applications and agents quickly. These new integrations can accelerate inferencing workloads for models available on Azure, providing significant performance improvements, greatly supporting the growing use of AI agents. Key features include optimized model throughput for NVIDIA accelerated computing platforms, prebuilt microservices deployable anywhere, and enhanced accuracy for specific use cases. In addition, we will soon be integrating the NVIDIA Llama Nemotron Reason open reasoning model. NVIDIA Llama Nemotron Reason is a powerful AI model family designed for advanced reasoning.   Epic, a leading electronic health record company, is planning to take advantage of the latest integration of NVIDIA NIM on Azure AI Foundry, improving AI applications to deliver better healthcare and patient results.  The launch of NVIDIA NIM microservices in Azure AI Foundry offers a secure and efficient way for Epic to deploy open-source generative AI models that improve patient care, boost clinician and operational efficiency, and uncover new insights to drive medical innovation. In collaboration with UW Health and UC San Diego Health, we’re also researching methods to evaluate clinical summaries with these advanced models. Together, we’re using the latest AI technology in ways that truly improve the lives of clinicians and patients. Drew McCombs, VP Cloud and Analytics, Epic  Further, Microsoft is also working closely with NVIDIA to optimize inference performance for popular, open-source language models and ensure they are available on Azure AI Foundry so customers can take full advantage of the performance and efficiency benefits from foundation models. The newest addition of this collaboration is the performance optimization for Meta Llama models using TensorRT-LLM. Developers can now use the optimized Llama models from the model catalog in Azure AI Foundry to experience improvements in throughput without additional steps.  “At Synopsys, we rely on cutting-edge AI models to drive innovation, and the optimized Meta Llama models on Azure AI Foundry have delivered exceptional performance. We’ve seen substantial improvements in both throughput and latency, allowing us to accelerate our workloads while optimizing costs. These advancements make Azure AI Foundry an ideal platform for scaling AI applications efficiently.” Arun Venkatachar, VP Engineering, Synopsys Central Engineering  At the same time, Microsoft is excited to be expanding its model catalog in Azure AI Foundry even further with the addition of Mistral Small 3.1, which is coming soon, an enhanced version of Mistral Small 3, featuring multimodal capabilities and an extended context length of up to 128k. Microsoft is also announcing the general availability of Azure Container Apps serverless graphics processing units (GPUs) with support for NVIDIA NIM. Serverless GPUs allow enterprises, startups, and software development companies to seamlessly run AI workloads on-demand with automatic scaling, optimized cold start, and per-second billing with scale down to zero when not in use to reduce operational overhead. With the support of NVIDIA NIM, development teams can easily build and deploy generative AI applications alongside existing applications within the same networking, security, and isolation boundary.   Expanding Azure AI Infrastructure with NVIDIA  The evolution of reasoning models and agentic AI systems is transforming the artificial intelligence landscape. Robust and purpose-built infrastructure is key to their success. Today, Microsoft is excited to announce the general availability of Azure ND GB200 V6 virtual machine (VM) series accelerated by NVIDIA GB200 NVL72 and NVIDIA Quantum InfiniBand networking. This addition to the Azure AI Infrastructure portfolio, alongside existing virtual machines that use NVIDIA H200 and NVIDIA H100 GPUs, highlight Microsoft’s commitment to optimizing infrastructure for the next wave of complex AI tasks like planning, reasoning, and adapting in real-time.   As we push the boundaries of AI, our partnership with Azure and the introduction of the NVIDIA Blackwell platform represent a significant leap forward. The NVIDIA GB200 NVL72, with its unparalleled performance and connectivity, tackles the most complex AI workloads, enabling businesses to innovate faster and more securely. By integrating this technology with Azure’s secure infrastructure, we are unlocking the potential of reasoning AI. Ian Buck, Vice President of Hyperscale and HPC, NVIDIA  The combination of high-performance NVIDIA GPUs with low-latency NVIDIA InfiniBand networking and Azure’s scalable architectures are essential to handle the new massive data throughput and intensive processing demands. Furthermore, comprehensive integration of security, governance, and monitoring tools from Azure supports powerful, trustworthy AI applications that comply with regulatory standards. Built with Microsoft’s custom infrastructure system and the NVIDIA Blackwell platform, at the datacenter level each blade features two NVIDIA GB200 Grace™ Blackwell Superchips and NVIDIA NVLink™ Switch scale-up networking, which supports up to 72 NVIDIA Blackwell GPUs in a single NVLink domain. Additionally, it incorporates the latest NVIDIA Quantum InfiniBand, allowing for scaling out to tens of thousands of Blackwell GPUs on Azure, providing two times the AI supercomputing performance from previous GPU generations based on GEMM benchmark analysis. As Microsoft’s work with NVIDIA continues to grow and shape the future of AI, the company also looks forward to bringing the performance of NVIDIA Blackwell Ultra GPUs and the NVIDIA RTX PRO 6000 Blackwell Server Edition to Azure. Microsoft is set to launch the NVIDIA Blackwell Ultra GPU-based VMs later in 2025. These VMs promise to deliver exceptional performance and efficiency for the next wave of agentic and generative AI workloads.   Azure AI’s infrastructure, advanced by NVIDIA accelerated computing, consistently delivers high performance at scale for AI workloads as evidenced by leading industry benchmarks like Top500 supercomputing and MLPerf results.1,2 Recently, Azure Virtual Machines using NVIDIA’s H200 GPUs achieved exceptional performance in the MLPerf Training v4.1 benchmarks across various AI tasks. Azure demonstrated leading cloud performance by scaling 512 H200 GPUs in a cluster, achieving a 28% speedup over H100 GPUs in the latest MLPerf training runs by MLCommons.3 This highlights Azure’s ability to efficiently scale large GPU clusters. Microsoft is excited that customers are utilizing this performance on Azure to train advanced models and get efficiency for generative inferencing.  Empowering businesses with Azure AI Infrastructure Meter is training a large foundation model on Azure AI Infrastructure to automate networking end-to-end. The performance and power of Azure will significantly scale Meter’s AI training and inference, aiding in the development of models with billions of parameters across text-based configurations, time-series telemetry, and structured networking data. With support from Microsoft, Meter’s models aim to improve how networks are designed, configured, and managed—addressing a significant challenge for progress. Black Forest Labs, a generative AI start-up with the mission to develop and advance state-of-the-art deep learning models for media, has extended its partnership with Azure. Azure AI services infrastructure is already being used to deploy its flagship FLUX models, the world’s most popular text-to-image media models, serving millions of high-quality images everyday with unprecedented speed and creative control. Building on this foundation, Black Forest Labs will adopt the new ND GB200 v6 VMs to accelerate the development and deployment of its next-gen AI models, pushing the boundaries of innovation in generative AI for media. Black Forest Labs has been a Microsoft partner since its inception, working together to secure the most advanced, efficient, and scalable infrastructure for training and delivering its frontier models.  We are expanding our partnership with Microsoft Azure to combine BFL’s unique research expertise in generative AI with Azure’s powerful infrastructure. This collaboration enables us to build and deliver the best possible image and video models faster and at greater scale, providing our customers with state-of-the-art visual AI capabilities for media production, advertising, product design, content creation and beyond.  Robin Rombach, CEO, Black Forest Labs  Creating new possibilities for innovators across industries Microsoft and NVIDIA have launched preconfigured NVIDIA Omniverse and NVIDIA Isaac Sim virtual desktop workstations, and Omniverse Kit App Streaming, on the Azure marketplace. Powered by Azure Virtual Machines using NVIDIA GPUs, these offerings provide developers everything they need to get started developing and self-deploying digital twin and robotics simulation applications and services for the era of physical AI. Several Microsoft and NVIDIA ecosystem partners including Bright Machines, Kinetic Vision, Sight Machine, and SoftServe are adopting these capabilities to build solutions that will enable the next wave of digitalization for the world’s manufacturers. There are many innovative solutions built by AI startups on Azure. Opaque Systems helps customers safeguard their data using confidential computing; Faros AI provides software engineering insights, allowing customers to optimize resources and enhance decision-making, including measuring the ROI of their AI coding assistants; Bria AI provides a visual generative AI platform that allows developers to use AI image generation responsibly, providing cutting-edge models trained exclusively on fully-licensed datasets; Pangaea Data is delivering better patient outcomes by enhancing screening and treatment at the point of care; and Basecamp Research is driving biodiversity discovery with AI and extensive genomic datasets.  Experience the latest innovations from Azure and NVIDIA  Today’s announcements at the NVIDIA GTC AI Conference underscore Azure’s commitment to pushing the boundaries of AI innovations. With state-of-the-art products, deep collaboration, and seamless integrations, we continue to deliver the technology that supports and empowers developers and customers in designing, customizing, and deploying their AI solutions efficiently. Learn more at this year’s event and explore the possibilities that NVIDIA and Azure hold for the future.  Visit us at Booth 514 at NVIDIA GTC.   Attend Microsoft sessions at NVIDIA GTC.   Explore Azure AI Foundry.   Sources: 1November 2024 | TOP500 2Benchmark Work | Benchmarks MLCommons 3Leading AI Scalability Benchmarks with Microsoft Azure – Signal65  The post Microsoft and NVIDIA accelerate AI development and performance   appeared first on Microsoft Azure Blog.",
            "url": "https://azure.microsoft.com/en-us/blog/microsoft-and-nvidia-accelerate-ai-development-and-performance/",
            "published_date": "2025-03-18T20:00:00Z",
            "processed": true,
            "phrase_refs": {
                "Microsoft Azure OpenAI Service": "https://azure.microsoft.com/products/ai-services/openai-service",
                "NVIDIA NIM microservices": "https://developer.nvidia.com/nim?sortBy=developer_learning_library%2Fsort%2Ffeatured_in.nim%3Adesc%2Ctitle%3Aasc",
                "Azure AI Foundry": "https://ai.azure.com/",
                "NBA": "https://www.microsoft.com/en/customers/story/19758-national-basketball-association-azure-open-ai-service",
                "BMW": "https://www.microsoft.com/en/customers/story/19769-bmw-ag-azure-app-service",
                "Dentsu": "https://www.microsoft.com/en/customers/story/19582-dentsu-azure-kubernetes-service",
                "Harvey": "https://www.microsoft.com/en/customers/story/19750-harvey-azure-open-ai-service",
                "OriGen": "https://www.microsoft.com/en/customers/story/1665511423001946809-OriGen-partner-professional-services-azure",
                "Come see Microsoft at the NVIDIA GTC AI Conference": "https://aka.ms/AzureBlog/AzureGTC25",
                "using TensorRT-LLM": "https://aka.ms/Nvidiatensorrtllmblog",
                "general availability of Azure Container Apps serverless graphics processing units (GPUs) with support for NVIDIA NIM": "https://aka.ms/NIMserverlessGPU",
                "general availability of Azure ND GB200 V6 virtual machine (VM) series": "https://aka.ms/GB200GA",
                "NVIDIA GB200 Grace™ Blackwell Superchips": "https://www.nvidia.com/en-us/data-center/gb200-nvl72/%22HYPERLINK%20%22https://www.nvidia.com/en-us/data-center/gb200-nvl72/%22HYPERLINK%20%22https://www.nvidia.com/en-us/data-center/gb200-nvl72/",
                "NVIDIA NVLink™ Switch": "https://www.nvidia.com/en-us/data-center/nvlink/",
                "NVIDIA Quantum InfiniBand": "https://www.nvidia.com/en-us/networking/quantum2/",
                "AI supercomputing performance": "https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/unpacking-the-performance-of-microsoft-azure-nd-gb200-v6-virtual-machines/4390442",
                "GEMM benchmark analysis": "https://github.com/Azure/AI-benchmarking-guide/tree/main/Azure_Results",
                "MLPerf Training v4.1 benchmarks": "https://github.com/mlcommons/training_results_v4.1/tree/main/Azure",
                "28% speedup over H100 GPUs": "https://aka.ms/Signal65WP_MLPerf",
                "Meter": "http://meter.com/",
                "Black Forest Labs": "https://blackforestlabs.ai/",
                "NVIDIA Omniverse": "https://www.nvidia.com/en-us/omniverse/%22HYPERLINK%20%22https://www.nvidia.com/en-us/omniverse/",
                "Isaac Sim": "https://developer.nvidia.com/isaac/sim",
                "Omniverse Kit App Streaming": "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/nvidia.ov_kit_app_streaming_application",
                "Bright Machines": "https://www.brightmachines.com/",
                "Kinetic Vision": "https://kinetic-vision.com/omniverse/",
                "Sight Machine": "https://sightmachine.com/",
                "SoftServe": "https://www.softserveinc.com/en-us",
                "Opaque Systems": "https://www.opaque.co/",
                "Faros AI": "https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.faros.ai%2F&data=05%7C02%7Cjasminechang%40microsoft.com%7C95b7f4b817744b00bf7208dd6578e114%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C638778292024407851%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=TtAXU3gk89Lh0V9L64IeXjFn8a%2F4Qp1OYMhPiegESMM%3D&reserved=0%22%20\\o%20%22Original%20URL:%20https://www.faros.ai/.%20Click%20or%20tap%20if%20you%20trust%20this%20link.",
                "Bria AI": "https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fbria.ai%2F&data=05%7C02%7Cjasminechang%40microsoft.com%7C95b7f4b817744b00bf7208dd6578e114%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C638778292024420331%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=Z1DO22qaZcaETfcOoA91js%2F8qrNy68eEq4UJ47o8nVY%3D&reserved=0%22%20\\o%20%22Original%20URL:%20https://bria.ai/.%20Click%20or%20tap%20if%20you%20trust%20this%20link.",
                "Pangaea Data": "https://www.pangaeadata.ai/",
                "Basecamp Research": "https://basecamp-research.com/",
                "Microsoft sessions at NVIDIA GTC": "https://aka.ms/TechComm/AzureGTC25",
                "November 2024 | TOP500": "https://www.top500.org/lists/top500/2024/11/",
                "Benchmark Work | Benchmarks MLCommons": "https://mlcommons.org/benchmarks/",
                "Leading AI Scalability Benchmarks with Microsoft Azure – Signal65": "https://signal65.com/research/ai/leading-ai-scalability-benchmarks-with-microsoft-azure/",
                "Microsoft and NVIDIA accelerate AI development and performance": "https://azure.microsoft.com/en-us/blog/microsoft-and-nvidia-accelerate-ai-development-and-performance/",
                "Microsoft Azure Blog": "https://azure.microsoft.com/en-us/blog"
            },
            "_rid": "JfZBAIxX5c4CAAAAAAAAAA==",
            "_self": "dbs/JfZBAA==/colls/JfZBAIxX5c4=/docs/JfZBAIxX5c4CAAAAAAAAAA==/",
            "_etag": "\"000080d3-0000-1500-0000-67e537680000\"",
            "_attachments": "attachments/",
            "ai_summary": "Microsoft and NVIDIA are enhancing their collaboration to drive AI innovations, particularly through the integration of NVIDIA's Blackwell platform with Azure AI services. New offerings include NVIDIA NIM microservices in Azure AI Foundry, which provide optimized containers for deploying generative AI applications, and the introduction of high-performance virtual machines designed for complex AI tasks. This partnership aims to empower developers and organizations across various sectors to leverage advanced AI capabilities for improved performance and efficiency.",
            "process_date": "2025-03-27T14:32:56.243219",
            "published": false,
            "publication_date": null,
            "_ts": 1743075176
        },
        {
            "id": "https___azure_microsoft_com_en-us_blog__p_39230",
            "title": "Cutting digital curbs: How Azure AI Foundry is building a more accessible world",
            "source": "https://azure.microsoft.com/en-us/blog/cutting-digital-curbs-how-azure-ai-foundry-is-building-a-more-accessible-world/",
            "content": "When my wife and I had our first child, I started seeing the world differently. Pushing a stroller through our neighborhood, I quickly realized how much I had taken for granted—sidewalks that suddenly ended, intersections without curb cuts, pathways that were technically walkable but not built for wheels. It wasn’t just a minor inconvenience. It made me think about my elderly grandmother, who relied on a walker. And my parents, who are active now but won’t always move as easily as they do today. Mobility and accessibility are deeply connected, and for the first time, I saw how infrastructure shapes our daily experiences—whether we notice it or not. But physical mobility is only part of the equation. In the digital world, there are just as many curbs that need cutting. Websites that don’t work with screen readers. Captions that lag behind real-time speech. AI models that fail to understand diverse voices. These barriers may be invisible to many, but they create real limitations for millions of people. And just like curb cuts in sidewalks, digital accessibility doesn’t just benefit one group—it makes technology better for everyone. That’s where Azure AI Foundry, Azure OpenAI Service, and the latest innovations in multimodal AI and Responsible AI (RAI) come in—helping organizations cut digital curbs and build a world that works for all.  As we recognize the impact of accessibility innovation at Microsoft’s Ability Summit 2025, we encourage you to explore how AI can drive greater inclusion in your products and services.  The goal isn’t just to eliminate obstacles—it’s to design a world where everyone moves forward together. Here are some of my favorite real-world examples.  Build with Azure AI Foundry today  Real-world impact: How Azure AI is cutting digital curbs The curb cuts of digital accessibility didn’t start with generative AI—Microsoft has been building inclusive technologies for decades. From early screen readers to speech-to-text innovations, AI has long played a pivotal role in expanding access. But now, we’re going even bigger.  Bridging the mental health gap with AI-powered conversations  Technology: Azure AI Mental health support is a growing necessity worldwide, but in Kenya, where there are only about 100 psychiatrists for a population of 50 million, access to professional care is extremely limited. Financial and cultural barriers often keep people from seeking the help they need. Kenya Red Cross saw an opportunity to bridge this gap using Azure AI-powered chatbots. In partnership with Pathways Technologies, they developed Chat Care, an AI-based mental health assistant that provides guidance, emotional support, and referrals—all in English and Swahili. This isn’t just a chatbot; it’s a lifeline for people who may otherwise suffer in silence. Chat Care allows users to start conversations about their mental health in a low-pressure, anonymous way, reducing stigma and offering resources that are accessible 24/7. It can suggest breathing exercises, gratitude practices, and in-person services, all tailored to the user’s responses. And for people who are deaf, hard of hearing, or unable to speak on the phone, Chat Care offers text-based support, ensuring mental health services are available to everyone, regardless of ability or circumstance. Improving AI speech recognition for non-standard speech Technology: Azure AI Speech x UIUC Partnership Voice recognition technology often struggles to understand people with non-standard speech patterns, making it harder for individuals with conditions like cerebral palsy or amyotrophic lateral sclerosis (ALS) to interact with AI-powered experiences. To solve this, Microsoft partnered with the University of Illinois Urbana-Champaign and fellow tech leaders to build the Speech Accessibility Project—a research initiative to train AI models that recognize diverse speech patterns. By integrating this breakthrough into Azure AI Speech, Microsoft is ensuring that AI-powered voice technology works for everyone, making digital experiences more inclusive across industries. Making AI more accessible from the ground up Technology: Azure AI Foundry With Azure AI Foundry, Microsoft has embedded accessibility into the AI development lifecycle itself. By partnering with EY, the Azure AI Foundry now empowers neurodivergent customers, and features improved usability, reducing cognitive overload and improving navigation for all people. In 2024, Azure AI Foundry reached a milestone for usability, reflecting feedback from people with disabilities that helped improve the platform. The updates included:  Grouping notifications and deployment errors to reduce cognitive overload.   Ensuring screen readers provide structured, easy-to-follow AI workflows.   Enhancing keyboard navigation for people who rely on shortcuts over mouse input.  This is a prime example of why accessibility is about building better, more intuitive technology for everyone. Making accessible AI work for agents Technology: Computer-Using Agent (CUA) Microsoft’s Computer-Using Agent (CUA) in Azure AI Foundry enables AI-powered automation of digital interactions, making software more accessible for people with limited mobility or dexterity. By allowing CUA to navigate interfaces, complete multi-step tasks, and execute actions through natural language commands, it reduces reliance on traditional keyboard and mouse inputs. This breakthrough enhances digital accessibility, empowering people who use any kind of assistive technology. As CUA dynamically interprets UI elements, it makes it easier to navigate applications and workflows. Hope, action, and moving forward together with Azure AI There are days when it feels like progress is slow. That accessibility, whether physical or digital, takes too long to improve. But then I think about something as simple as the sidewalks at my cross streets. Not that long ago, they were completely inaccessible. But after making a call, filing a report, and pushing the issue, those sidewalks finally got curb cuts just in time for the birth of our second child. It was a small fix in the grand scheme of things, but it made a real difference. The truth is, sometimes it just takes someone noticing the problem and taking action. But I also know I say that from a place of privilege—I had the time, the resources, and the ability to advocate for that change. Many people don’t. That’s why it’s so important that we build accessibility into our systems from the start—so that no one has to fight for the basics. With Azure AI Foundry, organizations can now scale accessibility faster than ever, making the digital world more navigable, usable, and welcoming to all. The curb cuts are being built—and the future of accessibility is wide open.  Join us at Ability Summit 2025 As we celebrate innovation at Microsoft’s Ability Summit 2025, we invite you to explore how AI can enhance accessibility in your products and services. The future isn’t just about removing barriers—it’s about building a world where everyone moves forward together.        Join us for the fifteenth annual Ability Summit The 2025 Ability Summit is a free digital event where you will find the latest in AI and accessibility         Click here to join >          The post Cutting digital curbs: How Azure AI Foundry is building a more accessible world appeared first on Microsoft Azure Blog.",
            "url": "https://azure.microsoft.com/en-us/blog/cutting-digital-curbs-how-azure-ai-foundry-is-building-a-more-accessible-world/",
            "published_date": "2025-03-17T15:00:00Z",
            "processed": true,
            "phrase_refs": {
                "Azure AI Foundry": "https://azure.microsoft.com/en-us/products/ai-foundry",
                "Azure OpenAI Service": "https://azure.microsoft.com/en-us/products/ai-services/openai-service",
                "multimodal AI": "https://azure.microsoft.com/en-us/products/ai-services",
                "Responsible AI (RAI)": "https://azure.microsoft.com/en-us/solutions/ai/responsible-ai-with-azure",
                "Ability Summit 2025": "https://abilitysummit.microsoft.com/",
                "Build with Azure AI Foundry today": "https://azure.microsoft.com/en-us/products/ai-foundry",
                "Kenya Red Cross": "https://www.microsoft.com/en/customers/story/19682-kenya-red-cross-society-azure-ai-services",
                "Speech Accessibility Project": "https://speechaccessibilityproject.beckman.illinois.edu/",
                "Azure AI Speech": "https://azure.microsoft.com/en-us/products/ai-services/ai-speech",
                "EY": "https://www.microsoft.com/en/customers/story/19581-ey-azure-ai-studio",
                "Computer-Using Agent (CUA)": "https://azure.microsoft.com/en-us/blog/announcing-the-responses-api-and-computer-using-agent-in-azure-ai-foundry/",
                "Click here to join >": "https://abilitysummit.microsoft.com/",
                "Cutting digital curbs: How Azure AI Foundry is building a more accessible world": "https://azure.microsoft.com/en-us/blog/cutting-digital-curbs-how-azure-ai-foundry-is-building-a-more-accessible-world/",
                "Microsoft Azure Blog": "https://azure.microsoft.com/en-us/blog"
            },
            "_rid": "JfZBAIxX5c4DAAAAAAAAAA==",
            "_self": "dbs/JfZBAA==/colls/JfZBAIxX5c4=/docs/JfZBAIxX5c4DAAAAAAAAAA==/",
            "_etag": "\"000081d3-0000-1500-0000-67e5376a0000\"",
            "_attachments": "attachments/",
            "ai_summary": "The article discusses the author's newfound awareness of mobility and accessibility issues after becoming a parent, highlighting how both physical and digital infrastructures often create barriers for individuals with disabilities. It emphasizes Microsoft's commitment to improving accessibility through innovations like Azure AI Foundry and various AI-powered tools that enhance digital experiences for everyone, including those with non-standard speech and mental health needs. Ultimately, the piece advocates for embedding accessibility into technology development from the outset to create a more inclusive world.",
            "process_date": "2025-03-27T14:32:58.108352",
            "published": false,
            "publication_date": null,
            "_ts": 1743075178
        },
        {
            "id": "https___azure_microsoft_com_en-us_blog__p_39119",
            "title": "Microsoft Cost Management updates—March 2025",
            "source": "https://azure.microsoft.com/en-us/blog/microsoft-cost-management-updates-march-2025/",
            "content": "  In this article        Optimizing AKS (Azure Kubernetes Service) costs AWS (Azure Web Services) connector deprecationExchange of Azure OpenAI Service Provisioned ReservationsHelp shape the future of cost reportingDocumentation updatesWhat's next for Cost Management?     Whether you’re a new student, a thriving startup, or the largest enterprise, you have financial constraints, and you need to know what you’re spending, where it’s being spent, and how to plan for the future. Nobody wants a surprise when it comes to the bill, and this is where Microsoft Cost Management comes in.  We’re always looking for ways to learn more about your challenges and how Microsoft Cost Management can help you better understand where you’re accruing costs in the cloud, identify and prevent bad spending patterns, and optimize costs to empower you to do more with less. Here are a few of the latest improvements and updates based on your feedback:   Manage your cloud cost with confidence — Microsoft Cost Management  Let’s dig into the details: Optimizing AKS (Azure Kubernetes Service) costs  Running your applications on AKS clusters can significantly reduce costs by optimizing resource utilization, enabling autoscaling, and facilitating shared infrastructure. AKS allows you to set resource limits, ensuring efficient use of resources and preventing over-provisioning. Its autoscaling features adjust the number of pods and nodes based on demand, so you only pay for what you need. By managing data transfer and inter-cluster communication efficiently, AKS helps lower networking costs. Additionally, you can run multiple applications on the same cluster, reducing the need for separate infrastructure.   Amplify human ingenuity  Microsoft Azure     In Cost analysis, you can use Kubernetes views to get visibility into the granular costs of your AKS clusters. The views provide you with visibility into the costs of namespaces running on your clusters and an aggregated view of costs of all resources running in your clusters. You simply need to install the cost analysis add-on to your cluster to enable this experience for your Azure subscriptions. These views also provide you with visibility into the Idle costs of your clusters. Idle costs, in simple terms, imply that you are not fully utilizing the capacity of your resources, resulting in under-utilization. Assets view showing Idle costs per asset:   Namespaces view showing Idle charges for the cluster:   The good news is that Azure provides you with different tools to minimize your Idle costs, and I strongly recommend you use the rich toolset for optimizing your AKS costs. You can set up your clusters and applications for autoscaling to minimize waste, use node auto provisioning for deciding the optimal VM configuration, and use Azure Monitor managed service for Prometheus to monitor utilization metrics. You can also optimize your VM costs by leveraging Spot VMs and taking advantage of savings instruments like Reservations and Savings Plans.    Best practices for cost optimization in AKS  Learn more     All the options available to you for optimizing AKS costs are explained very well in the article below: AWS (Azure Web Services) connector deprecation The Connector for AWS, once built to consolidate Microsoft Azure and AWS cloud cost data in Microsoft Cost Management, will be retired on March 31st, 2025. You will lose access to the connector, and AWS cost and usage data stored in the cost management service, including historical data, will be removed. Please note that we won’t be deleting the Cost and Usage Report (CUR) files you stored in your S3 bucket in the AWS console as part of the connector setup.  Please transition to your choice of cost management reporting for AWS and follow our instructions to delete your Connector to AWS in the Azure Portal. As covered in one of our previous blogs, exporting data in standard FOCUS (FinOps Cost and Usage Specification) format and using an analytics and reporting solution like Microsoft Fabric is a great way to analyze and report on costs from different sources.  Exchange of Azure OpenAI Service Provisioned Reservations As you perhaps already know, you can enjoy substantial discounts with Azure OpenAI provisioned reservations compared to standard pay-as-you-go pricing. You can lock in lower rates for dedicated capacity, ensuring you have the resources needed to run your powerful AI models. Last month (February 2025), for added flexibility to manage your reservations, we enabled exchange for provisioned reservations directly in the Azure portal. You still have the option to request a refund. To learn more about managing your reservations, please refer to this article. Help shape the future of cost reporting If you optimize cloud costs in your day-to-day work, from monitoring costs to managing commitments, we would love to hear from you. (All experience levels welcome!) Please take a few moments to complete this short, 5–10-minute survey to help us understand your roles, responsibilities, and what’s most important to you when optimizing in the cloud. Your feedback will help us improve our products and services to better meet your needs.  Click here to complete the Cost Optimization Survey  Documentation updates  Here are a few documentation updates you might be interested in:   New: Shared billing meter regions. New: Microsoft Entra ID free. Updated: View your usage summary details and download reports for EA enrollments. Updated: Save costs with Microsoft Fabric Capacity reservations. Want to keep an eye on all documentation updates? Check out the change history of the Cost Management and Billing documentation in the Azure Docs repository on GitHub. If you see something missing, select Edit at the top of the document and submit a quick pull request. You can also submit a GitHub issue. We welcome and appreciate all contributions!     What’s next for Cost Management? These are just a few of the big updates from last month. Don’t forget to check out the previous Microsoft Cost Management updates. We’re always listening and making constant improvements based on your feedback, so please keep the feedback coming.       Microsoft Cost Management Manage your cloud cost with confidence         Try now >          The post Microsoft Cost Management updates—March 2025 appeared first on Microsoft Azure Blog.",
            "url": "https://azure.microsoft.com/en-us/blog/microsoft-cost-management-updates-march-2025/",
            "published_date": "2025-03-12T15:00:00Z",
            "processed": true,
            "phrase_refs": {
                "Optimizing AKS (Azure Kubernetes Service) costs": "https://azure.microsoft.com/en-us/blog/feed/#optimizing-aks-azure-kubernetes-service-costs",
                "AWS (Azure Web Services) connector deprecation": "https://azure.microsoft.com/en-us/blog/feed/#aws-azure-web-services-connector-deprecation",
                "Exchange of Azure OpenAI Service Provisioned Reservations": "https://azure.microsoft.com/en-us/blog/feed/#exchange-of-azure-openai-service-provisioned-reservations",
                "Help shape the future of cost reporting": "https://azure.microsoft.com/en-us/blog/feed/#help-shape-the-future-of-cost-reporting",
                "Documentation updates": "https://azure.microsoft.com/en-us/blog/feed/#documentation-updates-1",
                "What's next for Cost Management?": "https://azure.microsoft.com/en-us/blog/feed/#what-s-next-for-cost-management-1",
                "Microsoft Cost Management": "https://azure.microsoft.com/services/cost-management/",
                "Manage your cloud cost with confidence — Microsoft Cost Management": "https://azure.microsoft.com/en-us/products/cost-management",
                "Microsoft Azure": "https://azure.microsoft.com/en-us/",
                "Cost analysis,": "https://aka.ms/costanalysis",
                "cost analysis add-on": "https://learn.microsoft.com/en-us/azure/aks/cost-analysis",
                "Learn more": "https://learn.microsoft.com/en-us/azure/aks/best-practices-cost",
                "our instructions": "https://review.learn.microsoft.com/en-us/azure/cost-management-billing/costs/retire-aws-connector?branch=pr-en-us-267352#delete-budgets-that-monitor-aws-charges",
                "previous blogs": "https://azure.microsoft.com/en-us/blog/microsoft-cost-management-updates-november-2024/#Exports-to-Microsoft-Fabric",
                "Microsoft Fabric": "https://www.microsoft.com/en-us/microsoft-fabric",
                "this article": "https://learn.microsoft.com/en-us/azure/cost-management-billing/reservations/azure-openai#cancel-exchange-or-refund",
                "Click here to complete the Cost Optimization Survey": "https://aka.ms/CostOptimizationSurvey",
                "Shared billing meter regions": "https://learn.microsoft.com/azure/cost-management-billing/understand/billing-meter-location",
                "Microsoft Entra ID": "https://learn.microsoft.com/azure/cost-management-billing/manage/microsoft-entra-id-free",
                "EA enrollments": "https://learn.microsoft.com/azure/cost-management-billing/manage/direct-ea-azure-usage-charges-invoices",
                "Microsoft Fabric Capacity": "https://learn.microsoft.com/azure/cost-management-billing/reservations/fabric-capacity",
                "Cost Management and Billing documentation": "https://github.com/MicrosoftDocs/azure-docs/commits/main/articles/cost-management-billing",
                "previous Microsoft Cost Management updates": "https://aka.ms/costmgmt/blog",
                "Try now >": "https://azure.microsoft.com/en-us/products/cost-management",
                "Microsoft Cost Management updates—March 2025": "https://azure.microsoft.com/en-us/blog/microsoft-cost-management-updates-march-2025/",
                "Microsoft Azure Blog": "https://azure.microsoft.com/en-us/blog"
            },
            "_rid": "JfZBAIxX5c4EAAAAAAAAAA==",
            "_self": "dbs/JfZBAA==/colls/JfZBAIxX5c4=/docs/JfZBAIxX5c4EAAAAAAAAAA==/",
            "_etag": "\"000082d3-0000-1500-0000-67e5376c0000\"",
            "_attachments": "attachments/",
            "ai_summary": "The article discusses recent updates and improvements in Microsoft Cost Management, focusing on optimizing costs for Azure Kubernetes Service (AKS) and the deprecation of the AWS connector. Key features include enhanced cost analysis tools for AKS that help users manage resource utilization, minimize idle costs, and leverage autoscaling and Spot VMs for cost efficiency. Additionally, the article highlights the introduction of Azure OpenAI Service provisioned reservations for better pricing flexibility and invites user feedback to further improve cost management services.",
            "process_date": "2025-03-27T14:32:59.369948",
            "published": false,
            "publication_date": null,
            "_ts": 1743075180
        },
        {
            "id": "https___azure_microsoft_com_en-us_blog__p_39100",
            "title": "Announcing the Responses API and Computer-Using Agent in Azure AI Foundry",
            "source": "https://azure.microsoft.com/en-us/blog/announcing-the-responses-api-and-computer-using-agent-in-azure-ai-foundry/",
            "content": "AI agents are transforming industries by automating workflows, enhancing productivity, and enabling intelligent decision-making. Businesses are leveraging AI agents to process insurance claims, manage IT service desks, optimize supply chain logistics, and even assist healthcare professionals in analyzing medical records. The potential is vast, and we’re excited to introduce two powerful innovations in Azure AI Foundry:  Responses API: A powerful API enabling AI-powered applications to retrieve information, process data, and take action seamlessly. Computer-Using Agent (CUA): A breakthrough AI model that navigates software interfaces, executes tasks, and automates workflows.  Together, these capabilities empower businesses to reimagine AI not just as an assistant—but as an active digital workforce. Enterprise customers will soon gain access to these innovations driving automation, efficiency, and intelligence at scale.   Enhancing AI Agents with the Responses API  The Responses API is the key to unlocking agentic AI in Azure AI Foundry, transforming how enterprises harness AI for real-world impact. It is the new foundation for leveraging Azure OpenAI Service’s powerful built-in tools, combining the simplicity of the Chat Completions API with the advanced capabilities available through Assistants API and Azure AI Agent Service. The Responses API enables seamless interaction with tools like CUA, function calling, and file search—all in a single API call. This API enables AI systems to retrieve data, process information, and take actions—seamlessly connecting agentic AI with enterprise workflows.   Build with Azure AI Foundry  How the Responses API Works  The Responses API provides a structured response format that allows AI to interact with multiple tools while maintaining context across interactions. It supports:   Tool calling in one simple API call: Now, developers can seamlessly integrate AI tools, making execution more efficient.    Computer use: Use the computer use tool within the Responses API to drive automation and execute software interactions.    File search: Interact with enterprise data dynamically and extract relevant information.    Function calling: Develop and invoke custom functions to enhance AI capabilities.    Chaining responses into conversations: Keep track of interactions by linking responses together using unique response IDs, ensuring continuity in AI-driven dialogues.    Enterprise-grade data privacy: Built with Azure’s trusted security and compliance standards, ensuring data protection for organizations.   By consolidating retrieval, reasoning, and action execution into a single API, the Responses API simplifies AI agent development, reducing the complexity of orchestrating multiple AI tools within an automation pipeline.  This scalability makes it well-suited for enterprise use cases across industries such as customer service, IT operations, finance, and supply chain management, where AI-powered automation can streamline workflows and improve efficiency. For even greater flexibility and control, organizations can explore Azure AI Agent Service, which offers additional tools and models for developing and scaling AI agents. Azure AI Agent Service integrates with Semantic Kernel and AutoGen, enabling seamless multi-agent orchestration for more complex scenarios requiring multiple agents to collaborate on tasks. Empowering AI Agents with the Computer-Using Agent The Computer-Using Agent (CUA) is a specialized AI model in Azure OpenAI Service that allows AI to interact with graphical user interfaces (GUIs), navigate applications, and automate multi-step tasks—all through natural language instructions. Unlike traditional automation tools that rely on predefined scripts or API-based integrations, CUA can interpret visual elements, adapt dynamically, and take action based on on-screen content.  Build custom generative AI solutions  What makes the Computer-Using Agent unique?  Autonomous UI navigation: Can open applications, click buttons, fill out forms, and navigate multi-page workflows. Dynamic adaptation: Interprets UI changes and adjusts actions accordingly, reducing reliance on rigid automation scripts. Cross-application task execution: Operates across web-based and desktop applications, integrating disparate systems without API dependencies. Natural language command interface: Users can describe a task in plain language, and CUA determines the correct UI interactions to execute.  With today’s announcement, developers can start building additional agentic capabilities right away with CUA. As enterprises look to deploy this technology at scale, we are evaluating integration with Windows 365 and Azure Virtual Desktop to enable CUA automation to run seamlessly in a managed host environment on Cloud PCs or virtual machines (VMs), ensuring consistent performance while maintaining enterprise compliance and security standards.  Secured desktops and apps with maximum control  Ensuring secure and trustworthy AI automation As AI systems become more autonomous, ensuring security, reliability, and alignment with human intent is critical. The CUA model is one of the first agentic AI models capable of directly interacting with software environments, bringing new challenges in misuse prevention, unintended actions, and adversarial risks. To address these, Microsoft and OpenAI have implemented a multi-layered safety approach spanning the model, system, and deployment levels. The CUA model is developed with safeguards to refuse harmful tasks, reject unauthorized actions, and prevent misuse. At the system level, Microsoft implements enterprise-grade content filtering and execution monitoring to help detect and prevent policy violations. To minimize unintended actions, CUA is designed to request user confirmations before executing irreversible tasks and to restrict high-risk actions such as financial transactions.  Microsoft’s Trustworthy AI framework further ensures real-time observability, logging, and compliance auditing for enterprise deployments. Automated and human-in-the-loop detection systems monitor execution patterns, identifying anomalous behaviors and enforcing governance policies. These safeguards are continuously refined based on internal red-teaming, external audits, and real-world testing to strengthen protection against prompt injections, adversarial manipulations, and unauthorized access. Given the current reliability level of the CUA model—particularly in non-browser environments—human oversight remains strongly recommended for sensitive operations. As AI agents evolve, Microsoft is committed to transparency, security, and ongoing risk mitigation. By combining CUA’s built-in safeguards with Azure’s enterprise compliance and governance tools, organizations can deploy AI-powered automation with confidence, ensuring safe and responsible AI adoption at scale.  Build and scale exceptional generative AI systems  Getting started with CUA and Responses API Azure AI Foundry continues to push the boundaries of AI-powered automation. Enterprise customers will gain access to the Responses API and CUA in Azure OpenAI Service in the coming weeks. We’re excited to see how developers and businesses innovate with these new capabilities.         Azure AI Foundry Innovate the future of AI using prebuilt and customizable elements.         Get started >           The post Announcing the Responses API and Computer-Using Agent in Azure AI Foundry appeared first on Microsoft Azure Blog.",
            "url": "https://azure.microsoft.com/en-us/blog/announcing-the-responses-api-and-computer-using-agent-in-azure-ai-foundry/",
            "published_date": "2025-03-11T20:30:00Z",
            "processed": true,
            "phrase_refs": {
                "Azure AI Foundry": "https://azure.microsoft.com/en-us/products/ai-foundry",
                "Build with Azure AI Foundry": "https://azure.microsoft.com/en-us/products/ai-foundry",
                "Azure AI Agent Service": "https://learn.microsoft.com/en-us/azure/ai-services/agents/overview",
                "Semantic Kernel and AutoGen": "https://techcommunity.microsoft.com/blog/educatordeveloperblog/using-azure-ai-agent-service-with-autogen--semantic-kernel-to-build-a-multi-agen/4363121",
                "Azure OpenAI Service": "https://azure.microsoft.com/en-us/products/ai-services/openai-service/",
                "Build custom generative AI solutions": "https://azure.microsoft.com/en-us/products/ai-services/openai-service",
                "Windows 365": "http://www.windows365.com/",
                "Azure Virtual Desktop": "https://azure.microsoft.com/en-us/products/virtual-desktop",
                "Secured desktops and apps with maximum control": "https://azure.microsoft.com/en-us/products/virtual-desktop",
                "Trustworthy AI": "https://blogs.microsoft.com/blog/2024/09/24/microsoft-trustworthy-ai-unlocking-human-potential-starts-with-trust/",
                "Build and scale exceptional generative AI systems": "https://azure.microsoft.com/en-us/solutions/ai",
                "Get started >": "https://azure.microsoft.com/en-us/products/ai-foundry",
                "Announcing the Responses API and Computer-Using Agent in Azure AI Foundry": "https://azure.microsoft.com/en-us/blog/announcing-the-responses-api-and-computer-using-agent-in-azure-ai-foundry/",
                "Microsoft Azure Blog": "https://azure.microsoft.com/en-us/blog"
            },
            "_rid": "JfZBAIxX5c4FAAAAAAAAAA==",
            "_self": "dbs/JfZBAA==/colls/JfZBAIxX5c4=/docs/JfZBAIxX5c4FAAAAAAAAAA==/",
            "_etag": "\"000083d3-0000-1500-0000-67e5376d0000\"",
            "_attachments": "attachments/",
            "ai_summary": "AI agents are revolutionizing various industries by automating workflows and enhancing decision-making capabilities, with significant applications in areas like insurance claims processing and healthcare. Microsoft has introduced two key innovations in Azure AI Foundry: the Responses API, which streamlines AI interactions and task execution, and the Computer-Using Agent (CUA), which allows AI to navigate software interfaces and perform multi-step tasks through natural language commands. These advancements aim to empower businesses to leverage AI as a dynamic digital workforce while ensuring security and compliance through robust safeguards.",
            "process_date": "2025-03-27T14:33:00.725119",
            "published": false,
            "publication_date": null,
            "_ts": 1743075181
        },
        {
            "id": "https___azure_microsoft_com_en-us_blog__p_39058",
            "title": "Microsoft named a leader in The Forrester Wave: Public Cloud Platforms, 2024",
            "source": "https://azure.microsoft.com/en-us/blog/microsoft-named-a-leader-in-the-forrester-wave-public-cloud-platforms-2024/",
            "content": "In 2010, we embarked on a journey to build a cloud platform that would empower organizations globally. In February, we celebrated 15 years of bringing Azure innovation to the world, and today, we celebrate Microsoft again being recognized as a leader in Forrester’s recently published report, The Forrester Wave™: Public Cloud Platforms, Q4 2024. We believe the recognition is a testament to the dedication of countless engineers, developers, and partners who have worked relentlessly to evolve Azure into the platform it is today—one that is trusted by enterprises to drive innovation, scale with confidence, and meet the challenges of an AI-driven world.  Recognized for vision Forrester’s evaluation assesses public cloud development and infrastructure platforms based on the strength of their current offerings and strategy. The Q4 2024 report underscores the strategic investments we’ve made to ensure Azure is not just a cloud provider but a comprehensive platform enabling real business impact. And today, that means helping organizations around the world balance migration, modernization, and the rapid advancement of AI—all at the same time. The reported cited key differentiators including Microsoft’s genAI strategy and Azure AI capabilities such as Azure OpenAI Service, Azure AI Studio, GitHub Copilot Enterprise and Microsoft Fabric, and noted that “Azure is a good fit for organizations seeking quick uptake of AI innovation as well as core cloud offerings for longstanding Microsoft environments.” Forrester’s acknowledgment reinforces our commitment to continuous innovation and customer success across all layers of the cloud—let’s take a closer look at why we believe Forrester recognized Microsoft as a leader.       Azure AI solutions Create the future with Azure AI Foundry         Try today >          AI at the Core: Powering innovation from cloud to edge Cloud computing has evolved from a solution for scaling infrastructure into a core enabler of intelligence and innovation. Similarly, AI has become an essential business tool to modernize, innovate, and remain competitive. Every app is being reinvented with AI, and entirely new applications are emerging that were never possible before. Simply put—AI is redefining how we work and how organizations operate. We deeply understand the challenges customers face as they navigate the balance between managing today’s demands and innovating for the future, which is why we’ve made AI a core part of our cloud strategy. By embedding AI across developer tools, platforms, data, and infrastructure, we are building a seamless foundation for the next generation of intelligent applications at scale. A key part of this approach is providing customers with choice and flexibility. For example, we offer one of the most comprehensive collections of open-source and foundation models, with more than 1,800 models available on Azure AI Foundry. In just the past two months, we’ve expanded our model catalog with more than 30 new models from leading partners and continued to advance our own Phi family of models. Across the AI space, our work spans multiple domains AI-powered app development We continue to invest in AI infrastructure, enterprise-grade AI services, and developer tools that drive real business impact, such as Azure AI Foundry and GitHub Copilot Enterprise, to transform how organizations build and deploy AI-powered solutions. Customers are seeing value in AI-infused application development, intelligent automation, and advanced analytics, with Azure providing the scalable infrastructure needed to integrate AI seamlessly across workloads—from modernizing business processes to accelerating software development. For example, via our strategic OpenAI partnership, we’re delivering their latest models as they launch in near real time, and with AOAI, responsible AI protection and capabilities are turned on by default. Azure AI Foundry Launched at Ignite in 2024, Azure AI Foundry is a unified platform that enables developers, IT administrators, data scientists, and others involved in AI development to design, customize, and manage AI applications and agents at scale. It seamlessly integrates with existing workbenches like GitHub, Visual Studio, and Copilot Studio, providing a rich set of AI capabilities and tools within familiar environments. With a simple portal for discovery and management, a unified SDK (software development kit) for agent toolchain access, and robust APIs (application programming interface), Azure AI Foundry streamlines AI development. It also offers secure data integration, model customization, and enterprise-grade governance to accelerate the path to production, and today, more than 60K customers are leveraging these features for their AI transformation. Enterprise-grade AI infrastructure Azure serves as the infrastructure that empowers AI—not only for today’s solutions but as the platform for tomorrow’s transformative AI applications. Our strategy is full-stack investment—from developer tools to AI platforms, data, and infrastructure. This comprehensive approach lays the foundation for the next generation of intelligent applications at scale. For instance, we’ve expanded our GPU capacity to support large-scale deep learning models, helping to ensure enterprise-grade reliability for AI workloads. Additionally, we are building the largest scale-up and scale-out infrastructure to support AI training and inferencing, leveraging new architectures like Grace Blackwell and high-throughput interconnects like NVLink and InfiniBand. With these new state-of-the-art technologies, customers get high throughput, reduced latency connectivity to Azure, and highly available specialized infrastructure that helps ensure their AI workloads run smoothly. Adaptive cloud approach enables flexibility across environments As Forrester highlights in their report, hybrid and multicloud capabilities are becoming the norm. Organizations operate in increasingly complex environments, requiring seamless integration across public cloud, hybrid, and edge infrastructure to optimize performance, enhance security, and drive innovation. Azure’s adaptive cloud strategy remains a key differentiator, enabling enterprises to unify their cloud operations while leveraging AI, applications, data, and infrastructure in a cohesive, scalable way:  Seamless hybrid and multicloud operations: Azure Arc extends Azure’s security, governance, and AI capabilities across on-premises, multicloud, and edge environments, providing businesses with a consistent platform for managing resources regardless of their location. AI-driven data and analytics: Microsoft Fabric integrates data, analytics, and AI-driven insights into a single solution, empowering organizations to make real-time, intelligent decisions across their entire data estate. Scalability across the intelligent edge: Azure’s AI-optimized infrastructure helps ensure organizations can deploy and run AI inferencing workloads at the edge with the same reliability, security, and performance as in the cloud.  With this approach, we help empower organizations to innovate and scale with agility through a cohesive platform that eliminates silos, accelerates innovation, and allows businesses to scale AI and applications seamlessly—no matter where they operate. Looking ahead with Azure AI This recognition from Forrester affirms our vision and continued investment in AI, app development, and enterprise-grade infrastructure. For the past 15 years, we’ve been dedicated to building and running the cloud platform that empowers businesses to innovate with confidence, and we’re grateful that Azure has become a trusted platform for all cloud workloads, evolving through continuous innovation and deep collaboration with customers and the Microsoft partner ecosystem. As we move forward, we remain committed to helping customers scale AI applications with Azure AI Foundry and Azure OpenAI Service, streamline development with GitHub Copilot and Azure Developer Services, and power their data and analytics strategies with tools like Microsoft Fabric. Our ever-expanding AI infrastructure, including increased GPU capacity, will continue to support advanced workloads, and businesses can continue to customize their cloud journey through our hybrid and multicloud solutions like Azure Arc, ensuring seamless operations across environments.  To read more about Forrester’s evaluation, visit the full report here. To learn more about our latest innovations in AI and cloud and how Azure can transform your business, visit our Azure page or explore our hybrid cloud solutions. The post Microsoft named a leader in The Forrester Wave: Public Cloud Platforms, 2024 appeared first on Microsoft Azure Blog.",
            "url": "https://azure.microsoft.com/en-us/blog/microsoft-named-a-leader-in-the-forrester-wave-public-cloud-platforms-2024/",
            "published_date": "2025-03-10T15:00:00Z",
            "processed": true,
            "phrase_refs": {
                "The Forrester Wave™: Public Cloud Platforms, Q4 2024": "https://reprint.forrester.com/reports/the-forrester-wave-tm-public-cloud-platforms-q4-2024-49fe4359/index.html",
                "Try today >": "https://azure.microsoft.com/en-us/solutions/ai",
                "Microsoft Fabric": "https://www.microsoft.com/en-us/microsoft-fabric",
                "Azure AI Foundry": "https://azure.microsoft.com/en-us/products/ai-foundry",
                "full report here": "https://reprint.forrester.com/reports/the-forrester-wave-tm-public-cloud-platforms-q4-2024-49fe4359/index.html",
                "Azure page": "https://azure.microsoft.com/en-us",
                "hybrid cloud solutions": "https://azure.microsoft.com/en-us/solutions/hybrid-cloud/",
                "Microsoft named a leader in The Forrester Wave: Public Cloud Platforms, 2024": "https://azure.microsoft.com/en-us/blog/microsoft-named-a-leader-in-the-forrester-wave-public-cloud-platforms-2024/",
                "Microsoft Azure Blog": "https://azure.microsoft.com/en-us/blog"
            },
            "_rid": "JfZBAIxX5c4GAAAAAAAAAA==",
            "_self": "dbs/JfZBAA==/colls/JfZBAIxX5c4=/docs/JfZBAIxX5c4GAAAAAAAAAA==/",
            "_etag": "\"000084d3-0000-1500-0000-67e5376e0000\"",
            "_attachments": "attachments/",
            "ai_summary": "Microsoft recently celebrated 15 years of Azure and was recognized as a leader in Forrester's Q4 2024 report on public cloud platforms. The report highlights Azure's comprehensive capabilities, including its AI-driven solutions and hybrid cloud strategies, which empower organizations to innovate and scale effectively in an increasingly AI-focused landscape. With features like Azure AI Foundry and enhanced infrastructure, Microsoft aims to support businesses in their AI transformation and operational efficiency across diverse environments.",
            "process_date": "2025-03-27T14:33:01.930715",
            "published": false,
            "publication_date": null,
            "_ts": 1743075182
        },
        {
            "id": "https___azure_microsoft_com_en-us_blog__p_38899",
            "title": "Optimizing incident management with AIOps using the Triangle System",
            "source": "https://azure.microsoft.com/en-us/blog/optimizing-incident-management-with-aiops-using-the-triangle-system/",
            "content": " High service quality is crucial to the reliability of the Azure platform and its hundreds of services. Continuously monitoring the platform service health enables our teams to promptly detect and mitigate incidents that may impact our customers. In addition to automated triggers in our system that react when thresholds are breached and customer-report incidents, we employ Artificial Intelligence-based Operations (AIOps) to detect anomalies. Incident management is a complex process, and it can be a challenge to manage the scale of Azure, and the teams involved to resolve an incident efficiently and effectively with the rich domain knowledge needed. I’ve asked our Azure Core Insights Team to share how they employ the Triangle System using AIOps to drive quicker time to resolution to ultimately benefit user experience. —Mark Russinovich, Azure CTO at Microsoft Optimizing incident management Incidents are managed by designated responsible individuals (DRIs) who are tasked with investigating incoming incidents to manage how and who needs to resolve the incident. As our product portfolio expands, this process becomes increasingly complex as the incident logged against a particular service may not be the root cause and could stem from any number of dependent services. With hundreds of services in Azure, it is nearly impossible for any one person to have domain knowledge in every area. This presents a challenge to the efficiency of manual diagnosis, resulting in redundant assignments and extended Time to Mitigate (TTM). In this blog, we’ll dive into how large language models, generative AI, and the Triangle System help us leverage automation and feedback loops for more efficient incident management.  Build in the cloud with Azure  AI agents are becoming more mature due to the improving reasoning ability of large language models (LLMs), enabling them to articulate all the steps involved in their thought processes. Traditionally, LLMs have been used for generative tasks like summarization without leveraging their reasoning capabilities for real-world decision-making. We saw a use case for this capability and built AI agents to make the initial assignment decisions for incidents, saving time and reducing redundancy. These agents use LLMs as their brain, allowing them to think, reason, and utilize tools to perform actions independently. With better reasoning models, AI agents can now plan more effectively, overcoming previous limitations in their ability to “think” comprehensively. This approach will not only improve efficiency but also enhance the overall user experience by ensuring quicker resolution of incidents. Introducing the Triangle System The Triangle System is a framework that employs AI agents to triage incidents. Each AI agent represents the engineers of a specific team and is encoded with domain knowledge of the team to triage issues. It has two advanced functions: Local Triage and Global Triage. Local Triage System The Local Triage System is a single agent framework that uses a single agent to represent each team. These single agents provide a binary decision to either accept or reject an incoming incident on behalf of its team, based on historical incidents and existing troubleshooting guides (TSGs). TSGs are a set of guidelines that engineers document to troubleshoot common patterns of issues. These TSGs are used to train the agent to accept or reject incidents and provide the reasoning behind the decision. Additionally, the agent can recommend the team to which the incident should be transferred to, based on the TSGs. As shown in Figure 1, the Local Triage system begins when an incident enters a service team’s incident queue. Based on the training from historical incidents and TSGs, the single agent employs Generative Pretrained Transformer (GPT) embeddings to capture the semantic meanings of words and sentences. Semantic distillation involves extracting semantic information from the incident that is closely related to incident being triaged. The single agent will then decide to accept or reject the incident. If accepted, the agent will provide the reasoning, and the incident will be handed off to an engineer to review. If rejected, the agent will either send it back to the previous team, transfer to a team indicated by the TSG, or keep it in the queue for an engineer to resolve.  Figure 1: Local Triage system workflow The Local Triage system has been in production in Azure since mid-2024. As of Jan 2025, 6 teams are in production with over 15 teams in the process of onboarding. The initial results are promising, with agents achieving 90% accuracy and one team saw a reduction in their TTM of 38%, significantly reducing the impact to customers. Global Triage System The Global Triage System aims to route the incident to the correct team. The system coordinates across all the single agents via a multi-agent orchestrator to identify the team that the incident should be routed to. As shown in Figure 2, the multi agent orchestrator selects suitable team candidates for the incoming incident, negotiates with each agent to find the correct team, further reducing TTM. This is a similar approach to patients coming into the emergency room, where the nurse briefly assesses symptoms and directs each patient to their specialist. As we further develop the Global Triage System, agents will continue to expand their knowledge and improve their decision-making abilities, greatly improving not only the user experience by mitigating customer issues quickly but also improving developer productivity by reducing manual toil.  Figure 2: Global Triage system workflow Looking forward We plan to expand coverage by adding more agents from different teams that will broaden the knowledge base to improve the system. Some of the ways we plan to do this include:  Extend the incident triage system to work for all teams: By extending the system to all teams, we aim to enhance the overall knowledge of the system enabling it to handle a wide range of issues. Creating a unified approach to incident management would lead to more efficient and consistent handling of incidents. Optimize the LLMs to swiftly identify and recommend solutions by correlating error logs with the specific code segments responsible for the issue: Optimizing LLMs to quickly identify, correlate, and recommend solutions will significantly speed up the troubleshooting process. It allows the system to provide precise recommendations, reducing the time engineers spend on debugging and leading to faster resolution of issues for customers. Expand auto mitigating known issues: Implementing an automated system to mitigate known issues will reduce TTM improving customer experience. This will also reduce the number of incidents that require manual intervention, enabling engineers to focus on delighting customers.  We first introduced AIOps as part of this blog series in February 2020 where we highlighted how integrating AI into Azure’s cloud platform and DevOps processes enhances service quality, resilience, and efficiency through key solutions including hardware failure prediction, pre-provisioning services, and AI-based incident management. AIOps continues to play a critical role today to predict, protect, and mitigate failures and impacts to the Azure platform and improve customer experience.  Unpack the Azure AI difference  By automating these processes, our teams are empowered to quickly identify and address issues, ensuring a high-quality service experience for our customers. Organizations looking to enhance their own service reliability and developer productivity can do so by integrating AI agents into their incident management processes designed in the Triangle System. Read the Triangle: Empowering Incident Triage with Multi-LLM-Agents paper from Microsoft Research.  Thank you to the Azure Core Insights and M365 Team for their contributions to this blog: Alison Yao, Data Scientist; Madhura Vaidya, Software Engineer; Chrysmine Wong, Technical Program Manager; Ze Li, Principal Data Scientist Manager; Sarvani Sathish Kumar, Principal Technical Program Manager; Murali Chintalapati, Partner Group Software Engineering Manager; Minghua Ma, Senior Researcher; and Chetan Bansal, Sr Principal Research Manager. The post Optimizing incident management with AIOps using the Triangle System appeared first on Microsoft Azure Blog.",
            "url": "https://azure.microsoft.com/en-us/blog/optimizing-incident-management-with-aiops-using-the-triangle-system/",
            "published_date": "2025-03-06T16:00:00Z",
            "processed": true,
            "phrase_refs": {
                "Build in the cloud with Azure": "https://azure.microsoft.com/en-us/pricing/purchase-options/azure-account/",
                "February 2020": "https://azure.microsoft.com/en-us/blog/advancing-azure-service-quality-with-artificial-intelligence-aiops/?msockid=3a4bce9c265d64423bf6dffd271e65cf",
                "Unpack the Azure AI difference": "https://azure.microsoft.com/en-us/solutions/ai",
                "Triangle: Empowering Incident Triage with Multi-LLM-Agents": "https://www.microsoft.com/en-us/research/publication/triangle-empowering-incident-triage-with-multi-llm-agents/",
                "Optimizing incident management with AIOps using the Triangle System": "https://azure.microsoft.com/en-us/blog/optimizing-incident-management-with-aiops-using-the-triangle-system/",
                "Microsoft Azure Blog": "https://azure.microsoft.com/en-us/blog"
            },
            "_rid": "JfZBAIxX5c4HAAAAAAAAAA==",
            "_self": "dbs/JfZBAA==/colls/JfZBAIxX5c4=/docs/JfZBAIxX5c4HAAAAAAAAAA==/",
            "_etag": "\"000085d3-0000-1500-0000-67e5376f0000\"",
            "_attachments": "attachments/",
            "ai_summary": "The article discusses the implementation of the Triangle System and Artificial Intelligence-based Operations (AIOps) to enhance incident management on the Azure platform. By utilizing AI agents that leverage large language models, Azure aims to streamline the triage process for incidents, improving efficiency and reducing resolution times significantly, with some teams reporting a 38% decrease in Time to Mitigate (TTM). The initiative focuses on automating incident assignments and routing, ultimately enhancing user experience and developer productivity while managing the complexities of Azure's extensive service portfolio.",
            "process_date": "2025-03-27T14:33:03.094124",
            "published": false,
            "publication_date": null,
            "_ts": 1743075183
        },
        {
            "id": "https___azure_microsoft_com_en-us_blog__p_38838",
            "title": "What’s new in Azure Elastic SAN",
            "source": "https://azure.microsoft.com/en-us/blog/whats-new-in-azure-elastic-san/",
            "content": "I’m excited to share our recent updates to Azure Elastic SAN—our solution for high-scale cost efficiency in the cloud. Whether you’re looking for a seamless migration of your SAN environment or looking to consolidate existing workloads within the cloud, this enterprise-class offering stands out by helping you simplify your storage management experience and giving you the optimal price-performance ratio for your workloads.    AZURE ELASTIC SAN  Cloud-native SAN, now in GA     With the SAN-like resource hierarchy, you can provision resources at the storage level and dynamically distribute these resources to meet the demands of diverse workloads across databases like SQL and Oracle, virtual desktop infrastructure (VDIs), and business applications. It supports a multitude of compute options, including workloads running on Azure VMware Solution, containerized applications (via Azure Container Storage), and virtual machines. Beyond that, it delivers cloud-native benefits with scale on demand, policy-based service management, and cloud-native security enforcements across encryption and network access. It’s a solution that combines the efficiency at scale of on-premises SAN systems and the flexibility of Cloud.   Deploy your Elastic SAN today  Since the general availability (GA) release of Azure Elastic SAN in early 2024, we have introduced various new capabilities that let you integrate more workloads, a few of which I want to highlight in this blog. Enhanced resiliency, scalability, and simplicity to empower mission-critical workloads We have released the public preview of autoscale for capacity. Elastic SAN is the first block storage solution in the cloud to support autoscaling. This will help save you time by simplifying management of the Elastic SAN, as you can set a policy for autoscaling your capacity when you are running out of storage rather than needing to actively track whether your storage is reaching its limits. With autoscaling, you can scale up on demand, so there is less of a need to provision extra storage just in case, helping you lower your monthly bill. Plus, you will be able to set the exact increments by which your SAN(s) will grow, so you stay in control of your total cost. It is a feature that fits in well with our theme of improving the ease of storage management.  We have also made snapshot support on Elastic SAN generally available. You can now take instant point-in-time backups of the state of your workloads with Elastic SAN volume snapshots. You can export these volume snapshots to managed disk snapshots for hardening purposes. Snapshots can be either full or incremental snapshots of your data, and you can restore your volumes from either of these snapshots in case you need to recover from a disaster. Additionally, we have enabled CRC protection to help you maintain the integrity of your data by providing CRC32C checksum verification. If enabled on the client side, Elastic SAN supports checksum verification at the volume group level. This will cause connections that don’t have CRC32C set for both header and data digests to be rejected, to prevent accidental errors during communication or storage of data.  All of this is backed by our published availability SLA of 99.99%, which covers mission critical workloads running on Elastic SAN. This is what will allow you to maintain your peace of mind while running these workloads.  Hosting SQL on Azure Virtual Machines (VMs) with Elastic SAN  We have dedicated ourselves to ensuring that the SQL experience on Elastic SAN has been fully validated and optimized for cost savings. We have verified that you can reliably use Elastic SAN for your clustered workloads like SQL FCI. This is especially helpful when paired with ZRS (zone-redundant storage) as you can ensure zonal redundancy for your failover clusters. Additionally, to assist with storage and workload consolidation of multi-SQL databases hosted on Elastic SAN, we built out Elastic SAN in such a way that you can get the performance you need while simultaneously improving your total cost of ownership with features like dynamic performance allocation. The performance you provision at the SAN level is shared across all the SAN volumes you create, and since all of your workloads won’t peak at the same time, you can avoid provisioning for the total peak performance target of all your workloads by utilizing this dynamic performance allocation. This helps with cost reduction as you no longer need to provision for your peak performance targets.  Reduced total cost of operations (TCO) for Azure VMware Solution with Elastic SAN  Azure Elastic SAN is now available as a storage option for all Azure VMware Solution SKUs, including the all new AV64 SKUs. Because Azure VMware Solution supports attaching iSCSI datastores as a persistent storage option, you can attach Elastic SAN volumes to an Azure VMware Solution cluster of your choice and present them as Virtual Machine File System (VMFS) datastores. By using VMFS datastores backed by Azure Elastic SAN, you can expand your storage with an Azure deployed, fully-managed and VMware Certified storage area network instead of scaling more local storage nodes in the cluster. Additionally, Elastic SAN is the only Azure block storage offering with ZRS capabilities that VMware customers can leverage. ZRS ensures high availability and resiliency by storing three copies of each SAN in three distinct and physically isolated storage clusters across different Azure availability zones.  Get fast migration for VMware workloads  Customers who are leveraging Azure VMware Solution for disaster recovery use cases or hosting capacity intensive workloads, can benefit from a more cost-efficient, extensible storage solution. For instance, you could stand up a AVS cluster as the secondary site for on-premises VMware environment, and replicate data into Elastic SAN while keeping minimal cluster footprint. At $0.06-0.08 per GiB per month (based on current pricing in East US1), Azure Elastic SAN is the most cost efficient per GiB storage option for AVS, while still offering scalable performance for a variety of use cases—and it can be deployed and connected straight from the Azure portal, making it an integrated and easy experience.  Enabling cloud-native workloads on Elastic SAN  For customers building their applications in the cloud, Azure Kubernetes Service (AKS) is the new VM. Azure Container Storage is Azure’s latest solution to simplify and offer comprehensive persistent storage management for stateful containers. Available as an easy add-on to your AKS cluster, one of the backing storage options you can use through Azure Container Storage is Elastic SAN. Azure Container Storage lets you provision persistent volumes (PVs) within a single Elastic SAN, sharing the performance available in the same SAN across multiple PVs. Because Elastic SAN utilizes iSCSI protocol to connect to AKS clusters, traditional VM-based limitations on the number of persistent volumes you can attach per node are overcome, reaching new heights of scale. Azure Container Storage can also be used with ZRS enabled, which offers enhanced resiliency for your workloads.  Traditional storage management for stateful containers meant provisioning storage resources, deploying PVs within your cluster, and manually managing the connection and scaling of the storage resource with your cluster. With Azure Container Storage, persistent volume orchestration and coordination of your Elastic SAN via your AKS cluster is fully managed for you. With the preview of autoscaling on Elastic SAN, storage management at scale is even simpler through Azure Container Storage. By setting your autoscale policy, you don’t need to worry about running into capacity limitations.   Manage volumes with Azure Container Storage  Getting started with Azure Elastic SAN  As always, there is more to come. Our roadmap for the year ahead involves expanding Elastic SANs existing backup and disaster recovery functionality, expanding our locally redundant storage (LRS) and ZRS footprint into new regions, and helping customers achieve even lower latencies and higher performance. For now, you can deploy an Elastic SAN2 by following our instructions on how to get started or refer to our documentation to learn more. If you have any questions or feedback, feel free to reach out to the PM team at AzElasticSAN-Ex@microsoft.com, and someone from our team will be happy to assist.        Azure Elastic SAN On-premises SAN capabilities in the cloud.         Try for free >           1You can find the most up to date pricing on our pricing page.  2 For a list of available regions please refer to our documentation.  The post What’s new in Azure Elastic SAN appeared first on Microsoft Azure Blog.",
            "url": "https://azure.microsoft.com/en-us/blog/whats-new-in-azure-elastic-san/",
            "published_date": "2025-03-05T16:00:00Z",
            "processed": true,
            "phrase_refs": {
                "Cloud-native SAN, now in GA": "https://azure.microsoft.com/en-us/products/storage/elastic-san/",
                "Azure VMware Solution": "https://learn.microsoft.com/en-us/azure/azure-vmware/introduction",
                "Azure Container Storage": "https://learn.microsoft.com/en-us/azure/storage/container-storage/",
                "Deploy your Elastic SAN today": "https://azure.microsoft.com/en-us/products/storage/elastic-san",
                "autoscale for capacity": "https://learn.microsoft.com/en-us/azure/storage/elastic-san/elastic-san-expand?tabs=azure-powershell-basesize%2Cazure-powershell-autoscale%2Cazure-powershell#autoscale-preview",
                "snapshot support": "https://learn.microsoft.com/en-us/azure/storage/elastic-san/elastic-san-snapshots?tabs=azure-portal%22%20\\o%20%22https://learn.microsoft.com/en-us/azure/storage/elastic-san/elastic-san-snapshots?tabs=azure-portal%22%20\\t%20%22_blank",
                "CRC protection": "https://learn.microsoft.com/en-us/azure/storage/elastic-san/elastic-san-networking-concepts#data-integrity",
                "our published availability SLA of 99.99%,": "https://www.microsoft.com/licensing/docs/view/Service-Level-Agreements-SLA-for-Online-Services?lang=1&year=2024",
                "SQL FCI": "https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/failover-cluster-instance-azure-elastic-san-manually-configure?view=azuresql",
                "storage option": "https://learn.microsoft.com/en-us/azure/azure-vmware/configure-azure-elastic-san",
                "Get fast migration for VMware workloads": "https://azure.microsoft.com/en-us/products/azure-vmware",
                "Azure Kubernetes Service": "https://azure.microsoft.com/en-us/products/kubernetes-service",
                "Manage volumes with Azure Container Storage": "https://azure.microsoft.com/en-us/products/container-storage",
                "new regions": "https://learn.microsoft.com/en-us/azure/storage/elastic-san/elastic-san-create?tabs=azure-portal#limitations",
                "how to get started": "https://learn.microsoft.com/en-us/azure/storage/elastic-san/elastic-san-create?tabs=azure-portal",
                "our documentation": "https://learn.microsoft.com/en-us/azure/storage/elastic-san/elastic-san-create?tabs=azure-portal",
                "AzElasticSAN-Ex@microsoft.com": "mailto:AzElasticSAN-Ex@microsoft.com",
                "Try for free >": "https://azure.microsoft.com/en-us/products/storage/elastic-san/",
                "pricing page": "https://azure.microsoft.com/en-us/pricing/details/elastic-san/",
                "documentation": "https://learn.microsoft.com/en-us/azure/storage/elastic-san/elastic-san-create?tabs=azure-portal#limitations",
                "What’s new in Azure Elastic SAN": "https://azure.microsoft.com/en-us/blog/whats-new-in-azure-elastic-san/",
                "Microsoft Azure Blog": "https://azure.microsoft.com/en-us/blog"
            },
            "_rid": "JfZBAIxX5c4IAAAAAAAAAA==",
            "_self": "dbs/JfZBAA==/colls/JfZBAIxX5c4=/docs/JfZBAIxX5c4IAAAAAAAAAA==/",
            "_etag": "\"000086d3-0000-1500-0000-67e537710000\"",
            "_attachments": "attachments/",
            "ai_summary": "Azure Elastic SAN has recently been updated to enhance its capabilities for high-scale cost efficiency in cloud storage, now generally available with features like autoscaling for capacity, snapshot support, and improved data integrity through CRC protection. This cloud-native SAN solution allows for dynamic resource allocation across various workloads, including SQL databases and virtual machines, while offering a cost-effective storage option for Azure VMware Solution users. With a focus on simplifying storage management and optimizing performance, Azure Elastic SAN aims to meet the needs of mission-critical applications in a flexible and efficient manner.",
            "process_date": "2025-03-27T14:33:04.401217",
            "published": false,
            "publication_date": null,
            "_ts": 1743075185
        },
        {
            "id": "https___azure_microsoft_com_en-us_blog__p_39048",
            "title": "Securing generative AI models on Azure AI Foundry",
            "source": "https://www.microsoft.com/en-us/security/blog/2025/03/04/securing-generative-ai-models-on-azure-ai-foundry/",
            "content": "New generative AI models with a broad range of capabilities are emerging every week. In this world of rapid innovation, when choosing the models to integrate into your AI system, it is crucial to make a thoughtful risk assessment that ensures a balance between leveraging new advancements and maintaining robust security. At Microsoft, we are focusing on making our AI development platform a secure and trustworthy place where you can explore and innovate with confidence.  Here we’ll talk about one key part of that: how we secure the models and the runtime environment itself. How do we protect against a bad model compromising your AI system, your larger cloud estate, or even Microsoft’s own infrastructure?   How Microsoft protects data and software in AI systems But before we set off on that, let me set to rest one very common misconception about how data is used in AI systems. Microsoft does not use customer data to train shared models, nor does it share your logs or content with model providers. Our AI products and platforms are part of our standard product offerings, subject to the same terms and trust boundaries you’ve come to expect from Microsoft, and your model inputs and outputs are considered customer content and handled with the same protection as your documents and email messages. Our AI platform offerings (Azure AI Foundry and Azure OpenAI Service) are 100% hosted by Microsoft on its own servers, with no runtime connections to the model providers. We do offer some features, such as model fine-tuning, that allow you to use your data to create better models for your own use—but these are your models that stay in your tenant.  So, turning to model security: the first thing to remember is that models are just software, running in Azure Virtual Machines (VM) and accessed through an API; they don’t have any magic powers to break out of that VM, any more than any other software you might run in a VM. Azure is already quite defended against software running in a VM attempting to attack Microsoft’s infrastructure—bad actors try to do that every day, not needing AI for it, and AI Foundry inherits all of those protections. This is a “zero-trust” architecture: Azure services do not assume that things running on Azure are safe!  What is Zero Trust?Learn more  Now, it is possible to conceal malware inside an AI model. This could pose a danger to you in the same way that malware in any other open- or closed-source software might. To mitigate this risk, for our highest-visibility models we scan and test them before release:   Malware analysis: Scans AI models for embedded malicious code that could serve as an infection vector and launchpad for malware.  Vulnerability assessment: Scans for common vulnerabilities and exposures (CVEs) and zero-day vulnerabilities targeting AI models.  Backdoor detection: Scans model functionality for evidence of supply chain attacks and backdoors such as arbitrary code execution and network calls.  Model integrity: Analyzes an AI model’s layers, components, and tensors to detect tampering or corruption.   You can identify which models have been scanned by the indication on their model card—no customer action is required to get this benefit. For especially high-visibility models like DeepSeek R1, we go even further and have teams of experts tear apart the software—examining its source code, having red teams probe the system adversarially, and so on—to search for any potential issues before releasing the model. This higher level of scanning doesn’t (yet) have an explicit indicator in the model card, but given its public visibility we wanted to get the scanning done before we had the UI elements ready.  Defending and governing AI models Of course, as security professionals you presumably realize that no scans can detect all malicious action. This is the same problem an organization faces with any other third-party software, and organizations should address it in the usual manner: trust in that software should come in part from trusted intermediaries like Microsoft, but above all should be rooted in an organization’s own trust (or lack thereof) for its provider.   For those wanting a more secure experience, once you’ve chosen and deployed a model, you can use the full suite of Microsoft’s security products to defend and govern it. You can read more about how to do that here: Securing DeepSeek and other AI systems with Microsoft Security. And of course, as the quality and behavior of each model is different, you should evaluate any model not just for security, but for whether it fits your specific use case, by testing it as part of your complete system. This is part of the wider approach to how to secure AI systems which we’ll come back to, in depth, in an upcoming blog.  Using Microsoft Security to secure AI models and customer data In summary, the key points of our approach to securing models on Azure AI Foundry are:   Microsoft carries out a variety of security investigations for key AI models before hosting them in the Azure AI Foundry Model Catalogue, and continues to monitor for changes that may impact the trustworthiness of each model for our customers. You can use the information on the model card, as well as your trust (or lack thereof) in any given model builder, to assess your position towards any model the way you would for any third-party software library.  All models hosted on Azure are isolated within the customer tenant boundary. There is no access to or from the model provider, including close partners like OpenAI.  Customer data is not used to train models, nor is it made available outside of the Azure tenant (unless the customer designs their system to do so).   Learn more with Microsoft Security To learn more about Microsoft Security solutions, visit our website. Bookmark the Security blog to keep up with our expert coverage on security matters. Also, follow us on LinkedIn (Microsoft Security) and X (@MSFTSecurity) for the latest news and updates on cybersecurity.  The post Securing generative AI models on Azure AI Foundry appeared first on Microsoft Azure Blog.",
            "url": "https://www.microsoft.com/en-us/security/blog/2025/03/04/securing-generative-ai-models-on-azure-ai-foundry/",
            "published_date": "2025-03-04T18:00:00Z",
            "processed": true,
            "phrase_refs": {
                "Azure AI Foundry": "https://azure.microsoft.com/products/ai-foundry",
                "Azure OpenAI Service": "https://azure.microsoft.com/products/ai-services/openai-service",
                "Azure Virtual Machines": "https://azure.microsoft.com/products/virtual-machines",
                "Learn more": "https://www.microsoft.com/security/business/zero-trust",
                "DeepSeek R1": "https://ai.azure.com/explore/models/DeepSeek-R1/version/1/registry/azureml-deepseek",
                "Securing DeepSeek and other AI systems with Microsoft Security": "https://www.microsoft.com/security/blog/2025/02/13/securing-deepseek-and-other-ai-systems-with-microsoft-security",
                "Azure AI Foundry Model Catalogue": "https://azure.microsoft.com/products/ai-model-catalog",
                "is not used to train models": "https://learn.microsoft.com/legal/cognitive-services/openai/data-privacy",
                "website.": "https://www.microsoft.com/en-us/security/business",
                "Security blog": "https://www.microsoft.com/security/blog/",
                "Microsoft Security": "https://www.linkedin.com/showcase/microsoft-security/",
                "@MSFTSecurity": "https://twitter.com/@MSFTSecurity",
                "Securing generative AI models on Azure AI Foundry": "https://www.microsoft.com/en-us/security/blog/2025/03/04/securing-generative-ai-models-on-azure-ai-foundry/",
                "Microsoft Azure Blog": "https://azure.microsoft.com/en-us/blog"
            },
            "_rid": "JfZBAIxX5c4JAAAAAAAAAA==",
            "_self": "dbs/JfZBAA==/colls/JfZBAIxX5c4=/docs/JfZBAIxX5c4JAAAAAAAAAA==/",
            "_etag": "\"000087d3-0000-1500-0000-67e537720000\"",
            "_attachments": "attachments/",
            "ai_summary": "Microsoft emphasizes the importance of security in the integration of generative AI models into AI systems, highlighting their commitment to a secure development platform. They ensure that customer data is not used to train shared models and that AI models are scanned for malware and vulnerabilities before release, employing a zero-trust architecture to protect against potential threats. Microsoft also offers tools and guidance for organizations to evaluate and secure AI models within their Azure AI Foundry environment.",
            "process_date": "2025-03-27T14:33:05.734420",
            "published": false,
            "publication_date": null,
            "_ts": 1743075186
        },
        {
            "id": "https___azure_microsoft_com_en-us_blog__p_38812",
            "title": "New global report: How to stand out in an AI-savvy world",
            "source": "https://azure.microsoft.com/en-us/blog/new-global-report-how-to-stand-out-in-an-ai-savvy-world/",
            "content": "What was the last thing you did with a generative AI app? Create a cat-unicorn coloring book for your niece? Summarize that 42-page  brief a colleague sent you? For me, it was using Microsoft Copilot to help my 9th grader with a history study session—I know more than you can believe about Mesopotamia. Whatever it was for you, I bet it was something you wouldn’t have even considered a year ago. As fast as we’ve become comfortable with AI at our fingertips, our expectations for what it can do for us are growing just as fast. Companies are responding to those rising expectations by increasingly customizing AI to create apps and unique experiences that differentiate their brand in the marketplace. When I say customers are customizing AI to create apps, I mean they are reshaping entire experiences with it. The NBA is redefining fandom with AI-powered personalization, delivering game highlights and stats tailored to each viewer. Meanwhile, the city of Buenos Aires has transformed urban living with ‘Boti,’ an AI chatbot managing over two million monthly queries, providing residents with instant assistance for things like driver’s license renewals, subway schedules, parking regulations, and even personalized tourism plans. These organizations are bending AI to their vision, pushing the limits of what’s possible. That is why I am happy to share a new MIT Technology Review Insights report that delves into how businesses are leveraging AI customization to stay ahead in the competitive market—DIY GenAI: Customizing generative AI for unique value. The report highlights the motivations, methods, and challenges faced by technology leaders as they tailor AI models to create net new value for their businesses.        Get the report Learn top techniques from 300 technical leaders who are differentiating their businesses through customized generative AI solutions.         Read here >          While AI customization isn’t new, rapidly advancing AI platforms like Azure AI Foundry can make it easier and offer businesses greater opportunities to create unique value with AI. According to the MIT report, while boosting efficiency is a top motivation for customizing generative AI models, creating unique solutions, better user satisfaction, and greater innovation and creativity are equal motivations. Improved efficiency is a top motivator here because it is the first clear-cut benefit businesses can realize quickly by customizing AI. As organizations gain experience, the learning curve flattens, and I think we’ll see the other motivators soar as companies focus more on customizing AI for top-line revenue impact than COGS (Cost of Goods Sold) savings.   Specializing with agents  When it comes to selecting models, half of the executives surveyed in the MIT report said they are prioritizing agentic and multi-agent capabilities in addition to multimodality (56%), flexible payment options (53%), and performance improvements (63%). AI agents that perform tasks and make decisions without the need for direct human intervention have broad utility. They lend themselves to autonomous problem solving in areas like data entry and retrieval for clinical operations in Healthcare, supplier coordination and maintenance tracking in manufacturing, and enhancing inventory and store operations in Retail. Agents have the potential to disrupt the market with something unique beyond automating processes that humans find dull. Take Atomicwork, a newcomer to the service management space dominated by established industry players with decades of experience. Atomicwork stands out with an ITSM (IT Service Management) and ESM (Enterprise Service Management) platform centered around specialized AI agents that integrate into the flow of work, providing seamless, instant support without the need for multiple tools or complex integrations. According to Atomicwork, one of their customers achieved a 65% deflection rate (the percentage of issues resolved without human intervention) within six months.  Like other areas of AI development, agent-building tools are rapidly evolving to accommodate a wide variety of use cases. From creating simple low-code agents in Microsoft Copilot Studio to developing more complex, autonomous pro-code agents using GitHub and Visual Studio, the process is streamlined. For example, using the intuitive agent orchestration experience built directly into Azure AI Foundry, Azure AI Agent Service allows you to accomplish in just a few lines of code what originally took hundreds of lines. This makes it remarkably easy to customize and safely put agents to work in your operations. Good data equals good AI  The potential of AI customization is immense but not without its challenges. Ironically, the greatest asset for AI customization often presents the biggest barrier customers run into: data. Specifically, data integrity—the safety, security, and quality of the data they use with AI. Half the participants in the MIT report cited data privacy and security (52%) and data quality and preparation (49%) as AI customization obstacles. Generative AI is one of the best things to happen to data in a long time. It presents innovative ways for companies to interact with and use their data in solutions unique to them. Data is where the magic happens. AI models know a lot, but a model doesn’t know your company from your competitor until you ground it in your data. Critical to empowering data-driven AI is an intelligent data platform that unifies sprawling, fragmented data stores, provides controls to govern and secure data, and seamlessly integrates with AI building tools. It’s why Microsoft Fabric is now the fastest-growing analytics product in our history and why we’re seeing AI-driven data growth of raw storage, database services, and app platform services as customers fuel their AI workloads with data. Fabric removes the data integrity obstacle. Together with Azure AI Foundry, data and dev teams are integrated and working in the same environment, removing any time-to-market drag due to data issues.   RAG is the customization starting point One of the simplest and most effective methods for customization is retrieval-augmented generation (RAG). Two-thirds of those surveyed in the MIT report are implementing RAG or exploring its use. Grounding an AI model in data specific to an organization or practice makes the model unique and capable of providing a specialized experience. In practice, RAG isn’t used alone to customize models. The report found it’s often used in combination with fine-tuning (54%) and prompt engineering (46%) to create highly specialized models. Dentsu, a global advertising and PR firm based in Tokyo, initially analyzed media channel contributions to client sales using general-purpose LLMs but found their accuracy lacking at 40-50%. To improve this, they developed custom data controls and structures and tailored models leveraging their expertise in retail and marketing data analysis. By integrating a customized RAG framework and an agentic decision layer, Dentsu reports about 95% accuracy in retrieving relevant data and insights. This AI-powered approach now plays a central role in shaping campaign strategies and optimizing marketing budget allocation for their clients.        Start building Design, customize, and manage exceptional AI apps with Azure AI Foundry, using tools you love across GitHub, Visual Studio, and Copilot Studio.         Try today >          Empowering development teams  Developing AI brings new dynamics, not the least of which is keeping pace with AI advancements. Model features and capabilities, along with developer tools and methods, are evolving rapidly, which makes empowering teams with the right tools crucial for successful AI customization.  For example, the pace of new model capabilities begs for model evaluation tooling automation. According to the MIT report, 54% of companies use manual evaluation methods, and 26% are either beginning to apply automated methods or are doing so consistently. I expect we’ll see these numbers flip soon. The report notes that playgrounds and prompt development features are also widely used to facilitate collaboration between AI engineers and app developers while customizing models. Evaluation is a critical component not just for customizing an AI but also in managing and monitoring the app once it hits production. We built full lifecycle evaluation into Azure AI Foundry so you can continuously evaluate model capabilities, optimize performance, test safety, and keep pace with advancements. We also see customization and growing AI portfolios ushering in next-generation AI development. The report reveals that more than half of the surveyed organizations have adopted telemetry tracing and debugging tools. AI tracing enhances the transparency needed to understand the outcomes of AI applications, and debugging helps optimize performance by showing how reasoning flows from the initial prompt to the final output.   Looking ahead with Azure AI AI has high utility when it comes to creating services and experiences that can differentiate you in the marketplace. The speed of adoption, exploration, and customization is evidence of the value companies see in that utility. Models are continually advancing and specializing by task and industry. In fact, there are more than 1,800 models in the Azure AI Foundry catalog today – and they are evolving just as quickly as the tools and methods to build with them. We already see agents delivering new customer service experiences—something that might be a differentiator today, but I expect fast-follows will reshape customer service for most companies as consumers learn to expect an AI-powered experience. As that happens, what we see as AI customization today will lose the novelty of being custom and become standard practice for building with AI. What we won’t lose is the novelty of building something unique. It will become an organization’s IP.  What’s that unique experience for your business? What’s the next special thing you want to do for your customers? How do you want to empower your employees? You’ll find everything you need to bend the curve of innovation with Azure AI Foundry.  One final note: No matter where you are in retooling your organization to operationalize AI, I encourage you to read the MIT report. In addition to survey findings, the team spent quality time talking with technology leaders about creating value by customizing generative AI. Sprinkled throughout the report are some helpful, real-world examples and insights. Big thanks to the researchers and editors at MIT Technology Review Insights for helping put a focus on this exciting area of opportunity.  Download the MIT report    About Jessica Hawk Jessica leads Azure Data, AI, and Digital Applications product marketing at Microsoft. Find Jessica’s blog posts here, and be sure to follow Jessica on LinkedIn. The post New global report: How to stand out in an AI-savvy world appeared first on Microsoft Azure Blog.",
            "url": "https://azure.microsoft.com/en-us/blog/new-global-report-how-to-stand-out-in-an-ai-savvy-world/",
            "published_date": "2025-03-04T16:00:00Z",
            "processed": true,
            "phrase_refs": {
                "NBA is redefining fandom": "https://www.microsoft.com/en/customers/story/19758-national-basketball-association-azure-open-ai-service?msockid=3b8626837d9d6e763c69323e7c286f90",
                "transformed urban living": "https://www.microsoft.com/en/customers/story/21596-government-of-the-city-of-buenos-aires-azure-open-ai-service",
                "DIY GenAI: Customizing generative AI for unique value": "https://info.microsoft.com/ww-landing-customizing-generative-ai-top-techniques-for-unique-value.html?LCID=EN-US",
                "Read here >": "https://info.microsoft.com/ww-landing-customizing-generative-ai-top-techniques-for-unique-value.html?LCID=EN-US",
                "Azure AI Foundry": "https://azure.microsoft.com/en-us/products/ai-foundry",
                "Atomicwork": "https://www.microsoft.com/en/customers/story/21861-atomicwork-azure-ai-foundry",
                "Azure AI Agent Service": "https://techcommunity.microsoft.com/blog/azure-ai-services-blog/unlocking-ai-powered-automation-with-azure-ai-agent-service/4372041",
                "Microsoft Fabric": "https://www.microsoft.com/en-us/microsoft-fabric",
                "Try today >": "https://azure.microsoft.com/en-us/products/ai-foundry",
                "Download the MIT report": "https://info.microsoft.com/ww-landing-customizing-generative-ai-top-techniques-for-unique-value.html?LCID=EN-US",
                "Jessica’s blog posts here": "https://azure.microsoft.com/en-us/blog/author/jessica-hawk/",
                "follow Jessica on LinkedIn": "https://www.linkedin.com/in/jessica-hawk-0686561/",
                "New global report: How to stand out in an AI-savvy world": "https://azure.microsoft.com/en-us/blog/new-global-report-how-to-stand-out-in-an-ai-savvy-world/",
                "Microsoft Azure Blog": "https://azure.microsoft.com/en-us/blog"
            },
            "_rid": "JfZBAIxX5c4KAAAAAAAAAA==",
            "_self": "dbs/JfZBAA==/colls/JfZBAIxX5c4=/docs/JfZBAIxX5c4KAAAAAAAAAA==/",
            "_etag": "\"000088d3-0000-1500-0000-67e537730000\"",
            "_attachments": "attachments/",
            "ai_summary": "The article discusses the rapid evolution and customization of generative AI applications in various sectors, highlighting how organizations like the NBA and Buenos Aires are leveraging AI to enhance user experiences and streamline operations. It emphasizes the importance of data integrity and innovative customization methods, such as retrieval-augmented generation (RAG), in creating unique AI solutions that drive efficiency and customer satisfaction. The article also references a new MIT Technology Review report that explores the motivations and challenges faced by businesses as they adopt tailored AI strategies to differentiate themselves in a competitive market.",
            "process_date": "2025-03-27T14:33:07.148263",
            "published": false,
            "publication_date": null,
            "_ts": 1743075187
        },
        {
            "id": "https___devblogs_microsoft_com__p_18608",
            "title": "Securing Developer Tools with Authentication Brokers",
            "source": "https://devblogs.microsoft.com/blog/securing-developer-tools-with-authentication-brokers",
            "content": "If you’ve used any applications that required connecting your personal Microsoft or Entra ID account, you’re probably familiar with the typical “authentication dance” – you see a browser window pop up, you enter your credentials, then you can close the browser, and the application is magically authenticated. The next application you launch does the same thing, and you need to go through the hassle of selecting your account, entering your credentials again, and then going back to your work. This has been the standard for quite some time, but it also lagged behind the modern security landscape. In the past couple of years, threat actors became more advanced and have started leveraging significantly more complex techniques to exfiltrate and misuse user credentials. Access and refresh tokens, the two credential artifacts that applications store after you sign in, became a prime target for those that want to steal personal and organizational data. To protect against both this and many other classes of emerging threats, we’ve been investing in making Entra ID-based authentication flows more robust, secure, and easier to use by driving adoption of authentication brokers. An authentication broker is an application that runs on a user’s machine that manages the authentication handshakes and token maintenance for connected accounts, such as your personal Microsoft or work and school Entra ID accounts. Unlike browser-based authentication flows, an authentication broker provides a much smoother user experience. Instead of sending the user out to sign in outside the app, they simply get a prompt to either select an existing account that is already connected or add a new account.  Once an account is connected to an authentication broker, it can be securely used across many other installed applications that need to sign users in without going through the trouble of re-entering the same credentials over and over. On Windows machines, the authentication broker is the Web Account Manager (WAM) – a feature of the operating system available in Windows 10 (Version 1703 – Creators Update) and above as well as Windows Server 2019 and above. Authentication brokers bring with them many benefits that go beyond an improved user experience, that include:  Enhanced security by default. As new security enhancements are developed, they will be seamlessly delivered with the broker, without developers needing to update their application logic. This significantly reduces the update and maintenance burden and ensures that customers get the best protection no matter what. Feature support. With the help of the broker, applications can natively access enhanced security capabilities such as Windows Hello, conditional access policies, and FIDO keys in a compliant and consistent manner. This means that organizational policies can be applied and respected by all client applications. System integration. Applications that use the broker plug-and-play with the built-in account picker, allowing the user to quickly pick an existing account instead of reentering the same credentials over and over. Once an account is connected, it can be easily used across other installed tools on the same computer. Token protection. WAM and other authentication brokers ensure that the refresh tokens are bound to the device, helping prevent attacks where a malicious actor exfiltrates user credentials and attempts to access data from another device. By default, authentication brokers ensure that the refresh tokens are protected and not accessible to the client application. And even in the worst case, a leaked refresh token is unusable outside the device it was issued on. Learn more in the Token Protection documentation.  Developer tool support for authentication brokers Our developer tools are designed for a wide range of professionals, including software engineers, IT administrators, data scientists, DevOps teams, and more. These individuals and their organizations require strong security measures within their development environments and tools to effectively build, deploy, and maintain the suite of applications and services. To ensure that their credentials are secure, we’ve started rolling out updates to Visual Studio, Visual Studio Code, Azure CLI, Azure PowerShell, and a range of our libraries, including Azure Identity and MSAL, to use authentication brokers such as WAM, as well as upcoming macOS and Linux brokers, out-of-the-box. In most of our tools running on Windows, WAM is already the default way to sign in with Microsoft personal and Entra ID accounts. This includes:  Visual Studio 2022, starting with version 17.11 Visual Studio Code, starting with version 1.97 Azure PowerShell, starting with version 12.0.0 Azure CLI, starting with version 2.61.0  As new authentication brokers are developed, we expect this pattern to become the norm across macOS and Linux systems as well. The change to use authentication brokers instead of browser-based authentication is entirely transparent to our customers and does not require any added work. When customers sign in, they will use the built-in OS account picker that will allow them to connect their accounts for single sign-on (SSO). We are excited about taking another step in securing our customers and their credentials. Feedback and support If you or your organization are encountering any issues with WAM-based authentication, refer to Microsoft Entra authentication and authorization error codes and Errors associated with Web Account Manager (WAM) articles to troubleshoot your issue. If you require additional support related to identity or account management, please report your issues in the Microsoft Support Community. If you are part of an organization that has a support contract with Microsoft, we recommend directly engaging with your designated support contact. The post Securing Developer Tools with Authentication Brokers appeared first on Microsoft for Developers.",
            "url": "https://devblogs.microsoft.com/blog/securing-developer-tools-with-authentication-brokers",
            "published_date": "2025-03-19T18:30:30Z",
            "processed": true,
            "phrase_refs": {
                "": "https://devblogs.microsoft.com/wp-content/uploads/2025/03/392bc24d-a24f-43ef-9dec-84da8c4a280c.gif",
                "Token Protection documentation": "https://learn.microsoft.com/en-us/entra/identity/conditional-access/concept-token-protection",
                "Visual Studio": "https://devblogs.microsoft.com/visualstudio/enhancing-your-visual-studio-authentication-experience/",
                "Visual Studio Code": "https://code.visualstudio.com/updates/v1_97#_microsoft-account-now-uses-msal-with-wam-support-on-windows",
                "Azure CLI": "https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli-interactively#sign-in-with-web-account-manager-wam-on-windows",
                "Azure PowerShell": "https://learn.microsoft.com/en-us/entra/identity-platform/scenario-desktop-acquire-token-wam",
                "Azure Identity": "https://devblogs.microsoft.com/azure-sdk/wham-authentication-broker-support-lands-in-the-azure-identity-libraries/",
                "MSAL": "https://devblogs.microsoft.com/identity/msal-net-wam/",
                "starting with version 17.11": "https://learn.microsoft.com/en-us/visualstudio/releases/2022/release-notes-v17.11#wamasdefault",
                "starting with version 1.97": "https://code.visualstudio.com/updates/v1_97#_microsoft-account-now-uses-msal-with-wam-support-on-windows",
                "starting with version 12.0.0": "https://learn.microsoft.com/en-us/powershell/azure/authenticate-interactive?view=azps-13.2.0#web-account-manager-wam",
                "starting with version 2.61.0": "https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli-interactively#sign-in-with-web-account-manager-wam-on-windows",
                "Microsoft Entra authentication and authorization error codes": "https://learn.microsoft.com/en-us/entra/identity-platform/reference-error-codes",
                "Errors associated with Web Account Manager (WAM)": "https://learn.microsoft.com/en-us/entra/msal/dotnet/advanced/exceptions/wam-errors",
                "Microsoft Support Community": "https://answers.microsoft.com/en-us",
                "Securing Developer Tools with Authentication Brokers": "https://devblogs.microsoft.com/blog/securing-developer-tools-with-authentication-brokers",
                "Microsoft for Developers": "https://devblogs.microsoft.com"
            },
            "_rid": "JfZBAIxX5c4LAAAAAAAAAA==",
            "_self": "dbs/JfZBAA==/colls/JfZBAIxX5c4=/docs/JfZBAIxX5c4LAAAAAAAAAA==/",
            "_etag": "\"000089d3-0000-1500-0000-67e537740000\"",
            "_attachments": "attachments/",
            "ai_summary": "The article discusses the transition from traditional browser-based authentication methods to using authentication brokers, specifically the Web Account Manager (WAM) on Windows, to enhance security and improve user experience when connecting to Microsoft or Entra ID accounts. Authentication brokers streamline the sign-in process by allowing users to select existing accounts or add new ones without repeatedly entering credentials, while also providing better protection for access and refresh tokens against credential theft. This shift is being implemented across various Microsoft development tools, ensuring that security updates are seamlessly integrated and that applications can utilize advanced security features.",
            "process_date": "2025-03-27T14:33:08.247078",
            "published": false,
            "publication_date": null,
            "_ts": 1743075188
        },
        {
            "id": "https___devblogs_microsoft_com__p_17965",
            "title": "Getting the most out of Azure DevOps and GitHub",
            "source": "https://devblogs.microsoft.com/blog/getting-the-most-out-of-azure-devops-and-github",
            "content": "Microsoft has two very successful DevSecOps products in the market – GitHub and Azure DevOps. Azure DevOps has a large enterprise customer base that loves the highly customizable enterprise-focused planning and tracking capabilities in Azure Boards, the robust continuous delivery capabilities in Azure Pipelines, the manual and exploratory testing capabilities in Azure Test Plans, and the deep integrations across the suite. GitHub is the world’s largest developer community, with over 100M developers. It also serves over 4M organizations, including 90% of the Fortune 100. It’s beloved by developers and at the forefront of innovation with features like GitHub Copilot, which is transforming every aspect of the software development process. Many Azure DevOps customers have been looking at the innovations coming out of GitHub and wondering how they can realize the benefits of those innovations while still using the capabilities they love in Azure DevOps. In the rest of this post, we’ll answer that question by looking at work we’ve been doing across Microsoft and GitHub to enable customers to acquire and use Azure DevOps and GitHub together to get the best of both worlds. GitHub innovation available to all Azure DevOps customers At last year’s Build conference, we announced GitHub Advanced Security for Azure DevOps (GHAzDO), which integrates the core capabilities of GitHub Advanced Security – secret scanning, code scanning, and dependency vulnerability scanning – directly into Azure DevOps. Since then, we’ve delivered a steady stream of improvements to GHAzDO, including (most recently) pull request annotations that highlight new code security and dependency vulnerabilities right in the Azure Repos pull request experience. In the coming months, our ongoing investments in GHAzDO will include secret validity (or “liveness”) checking, support for Dependabot auto-updates of dependency vulnerabilities, and more. GitHub Advanced Security for Azure DevOps pull request annotations Azure DevOps customers can similarly benefit from many of the key capabilities of GitHub Copilot for Business without making any other changes to their Azure DevOps usage. GitHub Copilot features like code completions, chat, extensions, and more are available directly in Visual Studio or Visual Studio Code. And Enterprise accounts can be used to manage GitHub Copilot licenses without any other GitHub Enterprise usage. Copilot Edits in Visual Studio Code           GitHub innovation available to Azure DevOps customers using GitHub repositories Of course, additional innovative capabilities in both GitHub Copilot and GitHub Advanced Security are available to customers who have their code in GitHub repositories. To better enable this for Azure DevOps customers without compromising their overall experience, we’ve been working hard across Microsoft and GitHub to improve the integrations between our two DevSecOps products. Our overarching goal is for the two products to feel like an integrated suite, with the same end-to-end traceability Azure DevOps customers have come to expect. In GitHub Copilot, many additional capabilities light up for customers whose code repositories are stored in GitHub, including codebase aware chat capabilities, pull request experiences like Copilot Workspace, and fine-tuned models. Similarly, GitHub Advanced Security (GHAS) has many capabilities that GitHub Advanced Security for Azure DevOps (GHAzDO) lacks. And while we will continue to invest in closing gaps between GHAzDO and GHAS, GHAS will always run ahead of GHAzDO. Today, this includes Copilot autofix capabilities and the new security campaigns features. Copilot autofix capabilities To take advantage of these capabilities in GitHub, Azure DevOps customers will need to migrate some or all their repositories to GitHub. To do this, we recommend:  Setting up a GitHub Enterprise organization. As a best practice, we recommend GitHub Enterprise Managed Users using the same Microsoft Entra tenant used by your Azure DevOps organization. With this configuration, you can use Entra groups to manage access to both organizations in a consistent way. Migrating repositories using GitHub Enterprise Importer. Start slow, and make sure to do a trial run or two to prove things out. Installing the Azure Boards and Azure Pipelines apps into your GitHub organization.  The Azure Boards app enables the same end-to-end traceability customers are used to when using all Azure DevOps – from ideas tracked in Boards all the way to production environments deployed by Azure Pipelines. And the Azure Pipelines app enables the same capabilities customers using Azure Repos are used to, including Pull Request triggers, Continuous Integration triggers, governed templates, and more. Many customers are already using these two apps to integrate Azure DevOps with GitHub. In October, customers:  Created more than 800 thousand links between Azure Boards and GitHub repositories (up 67% from last year), with the largest single customer creating more than 60 thousand. Ran more than 32 million Azure Pipelines jobs that consumed GitHub repositories (up 42%), with the largest single customer running more than 2 million.  Let’s take a quick tour of the integrated experience! It all starts by creating an initial link between an idea tracked in Azure Boards and the code changes that will bring that idea to life. This can be done either through a rich Boards user experience:   Create GitHub Branch from Azure Boards   Or by using AB# syntax in your commit message or PR description:  https://devblogs.microsoft.com/wp-content/uploads/2024/11/boards-github-short-demo.mp4 GitHub pull request link experience for Azure Boards Either way, you’ll get Azure Boards work item links in the Development section of your GitHub pull request, just as you would if you were using GitHub Issues. And you’ll see up-to-date status in your Azure Boards work item as your GitHub pull request gets updated. If you’ve configured PR triggers for your pipeline, you’ll see results show up right in the Checks experience within your pull request. And all the rich Azure Pipeline capabilities you expect will continue to be available, including governed templates, the newly announced Managed DevOps Pools, and more. We still have more planned work to further enhance the experience – see our public roadmap for the latest information. And as more customers adopt this approach, we will continue to learn from them and further integrate our two products. Licensing integration For some time now, Visual Studio subscribers have had usage rights for both GitHub Enterprise and Azure DevOps. But other users have had to pay for both products to use them together … until now! We are excited to announce that starting in January, we are including Azure DevOps Basic usage rights with GitHub Enterprise licenses and automating the experience for Azure DevOps customers. Just as with Visual Studio subscriptions, we will automatically detect GitHub Enterprise Licenses for users when they log into Azure DevOps and grant them a new GitHub Enterprise access level (with access equivalent to Azure DevOps Basic). License integration   This capability will begin lighting up for GitHub Enterprise Cloud customers in January, and for GitHub Enterprise Cloud with Data Residency customers early in the new year. Update: Originally, this blog post called out December, 2024 for when the licensing integration work would light up for GitHub Enterprise Cloud customers. We now expect the integration to light up in January. We are committed to enabling Azure DevOps customers to get the best out of both Azure DevOps and GitHub. We hope you’ll try out the latest integrations and innovations and let us know what you think!   The post Getting the most out of Azure DevOps and GitHub appeared first on Microsoft for Developers.",
            "url": "https://devblogs.microsoft.com/blog/getting-the-most-out-of-azure-devops-and-github",
            "published_date": "2024-11-19T13:30:53Z",
            "processed": true,
            "phrase_refs": {
                "GitHub": "https://github.com/home",
                "Azure DevOps": "https://azure.microsoft.com/en-us/products/devops/",
                "GitHub Copilot": "https://github.com/features/copilot",
                "GitHub Advanced Security for Azure DevOps": "https://azure.microsoft.com/en-us/products/devops/github-advanced-security",
                "": "https://devblogs.microsoft.com/wp-content/uploads/2024/11/license-integration.png",
                "Enterprise accounts": "https://docs.github.com/en/enterprise-cloud@latest/admin/copilot-business-only/about-enterprise-accounts-for-copilot-business",
                "GitHub Advanced Security": "https://github.com/advanced-security/",
                "GitHub Enterprise Managed Users": "https://docs.github.com/en/enterprise-cloud@latest/admin/managing-iam/understanding-iam-for-enterprises/about-enterprise-managed-users",
                "GitHub Enterprise Importer": "https://docs.github.com/en/migrations/using-github-enterprise-importer/migrating-from-azure-devops-to-github-enterprise-cloud",
                "Azure Boards": "https://github.com/apps/azure-boards",
                "Azure Pipelines": "https://github.com/apps/azure-pipelines",
                "https://devblogs.microsoft.com/wp-content/uploads/2024/11/boards-github-short-demo.mp4": "https://devblogs.microsoft.com/wp-content/uploads/2024/11/boards-github-short-demo.mp4",
                "newly announced Managed DevOps Pools": "https://devblogs.microsoft.com/devops/managed-devops-pools-ga/",
                "public roadmap": "https://learn.microsoft.com/en-us/azure/devops/release-notes/features-timeline#improved-boards--github-integration",
                "Getting the most out of Azure DevOps and GitHub": "https://devblogs.microsoft.com/blog/getting-the-most-out-of-azure-devops-and-github",
                "Microsoft for Developers": "https://devblogs.microsoft.com"
            },
            "_rid": "JfZBAIxX5c4MAAAAAAAAAA==",
            "_self": "dbs/JfZBAA==/colls/JfZBAIxX5c4=/docs/JfZBAIxX5c4MAAAAAAAAAA==/",
            "_etag": "\"00008ad3-0000-1500-0000-67e537750000\"",
            "_attachments": "attachments/",
            "ai_summary": "Microsoft's DevSecOps products, GitHub and Azure DevOps, are increasingly integrated to enhance user experience and innovation. Azure DevOps offers customizable enterprise solutions, while GitHub boasts a vast developer community and innovative features like GitHub Copilot. Recent enhancements, such as GitHub Advanced Security for Azure DevOps and improved licensing integration, aim to provide users with the benefits of both platforms while maintaining their preferred functionalities.",
            "process_date": "2025-03-27T14:33:09.206167",
            "published": false,
            "publication_date": null,
            "_ts": 1743075189
        },
        {
            "id": "https___devblogs_microsoft_com__p_17891",
            "title": "Software is a team sport: Building the future of software development together",
            "source": "https://devblogs.microsoft.com/blog/building-the-future-of-software-development-together",
            "content": "At Microsoft, we strive to build the world’s most beloved developer tools and services. Our vision is to empower every developer to transform their concepts into reality, from idea to code to cloud, at lightning speed. The Visual Studio family, Azure, GitHub, and GitHub Copilot empower developers around the world to do just that. Since GitHub joined Microsoft in 2018, we’ve been working hard to bring our tools and services together to deliver a productive and delightful experience for developers so they can focus on driving innovation through their code. We started by integrating features and services developers love and use every day from GitHub into our code editors. This includes creating and managing pull requests, issues, and repositories, and making it easier to deploy apps and services to Azure with GitHub Actions. We continue to innovate and collaborate to deliver an unparalleled end-to-end experience for developers and their teams. GitHub Copilot is the most adopted AI pair programming tool today with more than 1.8 million paid developers in addition to the 1 million students, teachers, and open-source maintainers that use GitHub for free. Copilot now makes it much easier to learn a new technology or programming language, ramp up on a codebase that’s new to you, or learn how to best configure your developer workstation. Let’s take a deeper look at how we’re building, innovating, and integrating GitHub Copilot throughout the entire development lifecycle into the tools and services you love. AI-powered end-to-end development platform GitHub Copilot: Code faster with the world’s most widely adopted AI developer tool Developing Copilot has been a cross-team effort and we’ve been relentlessly focused on building and releasing innovations to enhance the developer experience. Since the early days of code completions and contextual awareness, we’ve aimed to get to the heart of what developers can use to reduce toil and distraction and replace it with joy. In 2021, we started by bringing Copilot’s code completions into the Visual Studio family. Today, Copilot is capable of so much more than completion, which makes it an essential part of any developer’s setup. Use Copilot Chat to ask Copilot to explain, refactor, optimize, debug, document, and test your code. As you type, inline suggestions come to life, enabling you to convert comments to code, get help fixing errors, or let it write the test for the function you just wrote. Copilot Edits in Visual Studio Code As we continue to innovate together—we’re creating new ways to interact with chat. In addition to submitting prompts, developers can use keywords to help Copilot better understand your prompt, leverage slash commands to avoid writing complex prompts, and incorporate chat variables to specify context. Recently, we have added new support for multimodal modals, and the ability to speak directly to Copilot. https://devblogs.microsoft.com/wp-content/uploads/2024/11/speak.mp4 Perhaps most excitingly, AI model selection in Copilot unlocks a new world of possibilities giving developers the freedom to explore new models and select workable options. In Visual Studio Code, developers can access models from the model catalog like Anthropic’s Claude 3.5 Sonnet, Google’s Gemini 1.5 Pro, and OpenAI’s o1-preview and o1-mini. OpenAI and Claude 3.5 Sonnet models are available now; Google’s Gemini 1.5 Pro is coming soon. https://devblogs.microsoft.com/wp-content/uploads/2024/11/copilot-model-selection.mp4 GitHub Copilot continues to evolve with new capabilities for developers. With AI-powered code review, makes feedback from pull requests immediately actionable and easy to see in the context of your code editor.  With Copilot Extensibility, extensions can interface with services like Jira, Sentry, and more from the comfort of Copilot Chat in your favorite code editor, without having to disrupt your flow or context-switch. Best yet, you can create your very own extensions that integrate with your own data, services, and workflows into Copilot. GitHub Copilot Workspace in Visual Studio Code Finally, for task-focused scenarios, we’ve built Visual Studio Code extensions for the new GitHub Copilot Workspace so developers can get straight to the task at hand, jumpstarting the process by describing what they want in a natural language. Alongside Visual Studio Code, GitHub Copilot is deeply integrated into our entire developer tools ecosystem with handcrafted experiences and features based on the work that developers need to do within the tool of their choice. Visual Studio and GitHub Copilot Visual Studio is our premier IDE for .NET, C++, and game developers. In Visual Studio, developers build large-scale applications with sometimes a hundred projects for their distributed system. With GitHub Copilot, developers can index and search their entire codebase for deep insight, make code edits across multiple files and projects with the upcoming Edits feature, and get advanced visualizations and debugging insights to help them solve problems. For example, GitHub Copilot can be used with the Visual Studio profiler to identify and suggest fixes for a performance issue in the Visual Studio profiler codebase itself! https://devblogs.microsoft.com/wp-content/uploads/2024/11/visual-studio-profiler-copilot.mp4 GitHub Copilot has been infused throughout Visual Studio with new features, including the ability to set breakpoints, suggest code fixes, debug exceptions and tests, migrate projects, and much more.  With Visual Studio, .NET, and GitHub Copilot, solutions for some of the hardest challenges in building software at scale are now within reach of every developer. Working across GitHub Codespaces and Visual Studio Code Setting up a developer workstation is an arduous task, but we’ve built a seamless connection between Visual Studio Code and GitHub Codespaces to make it a breeze. You can use your local install of Visual Studio Code or one hosted on the web to create, manage, work in, and delete Codespaces all via the GitHub Codespaces extension. Dev containers are the common format for setting up both, which helped the GitHub team reduce their dev box setup time from 45 minutes to under a minute to onboard developers. We love how developers around the globe are using Codespaces for help with everyday development and how they’re being used to teach others to code with samples, workshops, and documentation. The near-instant coding environment has been integrated through all our training modules on Microsoft Learn and has become our standardized way to get started with learning any Microsoft technology. Better collaboration for Dev Teams Software is a team sport. In the last few years, dev teams using Azure DevOps have been seeking better integration with GitHub. Thousands of companies using Azure DevOps want to leverage the strength of both platforms and we’ve been working to make that possible. Developers and dev teams can now use GitHub Advanced Security for Azure DevOps and GitHub Copilot for Business within Azure DevOps. Secure your Azure Repos with advanced scanning features and harness the AI-driven coding assistance of GitHub Copilot, all without leaving your current workflow. Streamlined Integration and Migration We’re also focused on improving the integration between Azure Boards, Azure Pipelines, and GitHub Repositories. This work will empower dev teams to fully leverage the advanced features of Copilot and GitHub Advanced Security, while leveraging enterprise-focused project and tracking capabilities in Azure Boards, and the robust continuous delivery capabilities in Azure Pipelines. Developers can now create GitHub branches directly from Azure Boards, find Azure Boards work item links in GitHub pull requests, trigger Azure Pipeline jobs at pull request time or after changes are merged, and more. Our goal is to provide developers using all of these products with the same end-to-end traceability they’ve come to expect from Azure DevOps. With the GitHub Enterprise Importer, developers can transition their repos from Azure repos to GitHub repos all while continuing to use Azure Boards and Azure Pipelines, and access GitHub’s powerful capabilities in AI and security. https://devblogs.microsoft.com/wp-content/uploads/2024/11/boards-github-short-demo.mp4 Simplifying cloud deployments We want to make the path to Azure easier and more intuitive for developers. To do that, we’re building better integration with GitHub. Learn how to build and deploy to the cloud with GitHub Copilot for Azure GitHub Copilot makes it easier to learn how to build and deploy to the cloud and how to do it without friction safely from the cockpit of where your code lives. We recently launched the preview of GitHub Copilot for Azure, which builds upon the Copilot Chat capabilities in Visual Studio Code to help developers manage resources and deploy applications. Using GitHub Copilot for Azure, developers can get personalized guidance to learn, provision, deploy, diagnose, and estimate costs for Azure services and resources without needing to leave the comfort of their personalized code editor. Using GitHub Copilot for Azure in Visual Studio Code Seamless Azure CI/CD integration with GitHub Actions GitHub Actions for Azure helps you create workflows that build, test, package, release, and deploy to Azure for CI/CD integration. GitHub Actions also includes support for key utilities that can streamline testing and deployment of intelligent apps with a variety of AI models, including Azure Resource Manager templates, Azure CLI, and Azure Policy. The Azure Developer CLI (azd) gives developers a direct path in the dev loop to provision Azure resources, deploy, and set up a CI/CD pipeline for continuous deployments. GitHub Actions workflows are now automatically generated when creating apps with azd templates and infrastructure for popular services, including Azure App Service and Static Web Apps. Creation of GitHub Actions with azd To help start you on your AI app development journey, we’ve launched the AI app templates gallery with a curated collection of templates that can be deployed to Azure using Visual Studio Code or GitHub Codespaces. These templates are completely open source on GitHub for you to contribute your own ideas. AI App Template gallery Tools for building your own intelligent apps The AI field is evolving rapidly, and it can be hard to keep up. GitHub Models supports secure experimentation with models, providing QuickStart code for various languages and frameworks to simplify model exploration. You can seamlessly transition from GitHub Models to Codespaces, Visual Studio Code, and Azure as you go from experiment to prototype to production deployment. If you’re not sure where to start or want to test drive, you can experiment in the GitHub playground for free and scale your AI apps to paid endpoints using Azure for secure, enterprise-level deployment and monitoring. You can now also scale your AI applications with key insights leveraging Azure AI evaluation and online experimentation Actions (both in preview). These capabilities can be fully integrated into your CI/CD development workflows with pre-production evaluation and online experimentation results posted directly back to GitHub for analysis. https://devblogs.microsoft.com/wp-content/uploads/2024/11/genai-evaluations-2.mp4 Self-provisioning GitHub from the Azure portal Procurement of tools and infrastructure should be simple and easily accessible. You can now purchase and self-provision GitHub directly from the Azure portal, including setting up an Enterprise Managed Users configuration. Speaking of Azure, the new GitHub Enterprise Cloud with data residency is built on top of Azure, which means that you get the security, business continuity, and disaster recovery capabilities of the cloud. Developers build our future As we look ahead, the future of software development has never been brighter. Microsoft and GitHub are committed to empowering developers around the world to innovate, collaborate, and create solutions that’ll shape the next generation of technology. By combining our strengths, we’re paving the way for a more connected and dynamic digital landscape. We’re excited to continue this journey with you and we can’t wait to see what amazing things we’ll build together. We’ve just launched the Microsoft for Developers blog, where we’ll strive to tell the full end-to-end story for developers as they build and infuse their applications with AI, migrate their applications to the cloud, or take advantage of the latest features and platform capabilities. To stay up to date with the new Microsoft for Developers blog, subscribe in your favorite RSS reader or by signing up for email notifications whenever we publish a new post. The post Software is a team sport: Building the future of software development together appeared first on Microsoft for Developers.",
            "url": "https://devblogs.microsoft.com/blog/building-the-future-of-software-development-together",
            "published_date": "2024-11-18T17:00:08Z",
            "processed": true,
            "phrase_refs": {
                "most adopted AI pair programming tool today": "https://survey.stackoverflow.co/2024/technology#1-ai-search-and-developer-tools",
                "": "https://devblogs.microsoft.com/wp-content/uploads/2024/11/ai-app-templates.png",
                "building and releasing innovations": "https://github.blog/ai-and-ml/github-copilot/",
                "Copilot Chat": "https://docs.github.com/copilot/using-github-copilot/asking-github-copilot-questions-in-your-ide",
                "https://devblogs.microsoft.com/wp-content/uploads/2024/11/speak.mp4": "https://devblogs.microsoft.com/wp-content/uploads/2024/11/speak.mp4",
                "AI model selection in Copilot": "https://github.blog/news-insights/product-news/bringing-developer-choice-to-copilot",
                "https://devblogs.microsoft.com/wp-content/uploads/2024/11/copilot-model-selection.mp4": "https://devblogs.microsoft.com/wp-content/uploads/2024/11/copilot-model-selection.mp4",
                "extensions": "https://github.com/features/copilot/extensions",
                "GitHub Copilot Workspace": "https://marketplace.visualstudio.com/items?itemName=GitHub.copilot-workspace",
                "Visual Studio": "https://www.visualstudio.com",
                "https://devblogs.microsoft.com/wp-content/uploads/2024/11/visual-studio-profiler-copilot.mp4": "https://devblogs.microsoft.com/wp-content/uploads/2024/11/visual-studio-profiler-copilot.mp4",
                "infused throughout Visual Studio": "https://www.youtube.com/watch?v=iYDRmh5-D7U",
                "GitHub Codespaces extension": "https://marketplace.visualstudio.com/items?itemName=GitHub.codespaces",
                "Microsoft Learn": "https://learn.microsoft.com/training/browse/",
                "Azure Boards": "https://learn.microsoft.com/en-us/azure/devops/boards/github/?view=azure-devops",
                "GitHub Enterprise Importer": "https://docs.github.com/en/migrations/using-github-enterprise-importer",
                "https://devblogs.microsoft.com/wp-content/uploads/2024/11/boards-github-short-demo.mp4": "https://devblogs.microsoft.com/wp-content/uploads/2024/11/boards-github-short-demo.mp4",
                "preview of GitHub Copilot for Azure": "https://code.visualstudio.com/blogs/2024/11/15/introducing-github-copilot-for-azure",
                "GitHub Actions for Azure": "https://learn.microsoft.com/en-us/azure/developer/github/github-actions",
                "AI app templates gallery": "https://azure.github.io/ai-app-templates/",
                "open source on GitHub": "https://azure.github.io/awesome-azd/",
                "scale your AI apps to paid endpoints using Azure": "https://learn.microsoft.com/azure/ai-studio/ai-services/how-to/quickstart-github-models",
                "preview": "https://aka.ms/genAI-CI-CD-private-preview",
                "https://devblogs.microsoft.com/wp-content/uploads/2024/11/genai-evaluations-2.mp4": "https://devblogs.microsoft.com/wp-content/uploads/2024/11/genai-evaluations-2.mp4",
                "Microsoft for Developers": "https://devblogs.microsoft.com",
                "Software is a team sport: Building the future of software development together": "https://devblogs.microsoft.com/blog/building-the-future-of-software-development-together"
            },
            "_rid": "JfZBAIxX5c4NAAAAAAAAAA==",
            "_self": "dbs/JfZBAA==/colls/JfZBAIxX5c4=/docs/JfZBAIxX5c4NAAAAAAAAAA==/",
            "_etag": "\"00008bd3-0000-1500-0000-67e537780000\"",
            "_attachments": "attachments/",
            "ai_summary": "Microsoft aims to create the world's most beloved developer tools, empowering developers to quickly transform ideas into reality through integrated services like Visual Studio, Azure, GitHub, and GitHub Copilot. Since acquiring GitHub in 2018, Microsoft has focused on enhancing developer experiences by integrating popular features, improving collaboration, and leveraging AI capabilities in tools like GitHub Copilot, which now supports a wide range of functionalities from code completion to debugging and cloud deployment. The ongoing innovations aim to streamline workflows and foster collaboration among development teams, ultimately shaping the future of software development.",
            "process_date": "2025-03-27T14:33:11.697002",
            "published": false,
            "publication_date": null,
            "_ts": 1743075192
        },
        {
            "id": "https___devblogs_microsoft_com__p_17735",
            "title": "Welcome to the Microsoft for Developers blog",
            "source": "https://devblogs.microsoft.com/blog/welcome-to-the-microsoft-for-developers-blog",
            "content": "Did you know that developers use an average of 16 tools per day? A typical developer works across their code editor, a terminal, several command-line tools and web portals – not to mention all the SDKs and packages they take a dependency on in their code. Sometimes it can be challenging to figure out how to make it all work together. If you’re tasked with providing guidance and common infrastructure for other developers on your team, you may need to wrestle with many more tools and services. Whether you use the Visual Studio family of IDEs, GitHub, Azure DevOps or build apps for Azure, Windows, M365, Teams or Microsoft Copilot, now more than ever, we need a place to share with you how you can use our products together so you can focus on writing the code that’s unique to your organization. Microsoft for Developers will strive to tell the full end-to-end story for developers as they build and infuse their applications with AI, migrate their applications to the cloud, or take advantage of the latest features and platform capabilities. You’ll find updates on how to become more productive than ever as we integrate all of our tools and services with GitHub Copilot and surface the knowledge in the code editors that you use every day. Developer blogs here at Microsoft are trusted by millions of developers to get deep insight and the latest news about the products they love – directly from the product teams at Microsoft that build them. Our plan is to not change anything about the current developer blog ecosystem that developers trust. You’ll still get all of your product-specific updates and insights from blogs such as .NET, Visual Studio, VS Code, GitHub, and so many more. Think of this new blog as a unified hub for major announcements that span multiple products and deep insights into making them all work for you. Often adopting new technology requires culture change in the form of upskilling, changing organizational structure or job content. We’ll share insights and best practices for DevSecOps, platform engineering, cloud-native architectures, building with AI, and enterprise integration. Finally, we’ll share your stories about how you’re using these products and services together to build world class applications and services to empower people around the world to achieve more. To stay up to date with the new Microsoft for Developers blog, subscribe in your favorite RSS reader or by signing up for e-mail notifications whenever we release a new blog. #HappyCoding     The post Welcome to the Microsoft for Developers blog appeared first on Microsoft for Developers.",
            "url": "https://devblogs.microsoft.com/blog/welcome-to-the-microsoft-for-developers-blog",
            "published_date": "2024-11-11T16:56:11Z",
            "processed": true,
            "phrase_refs": {
                "16 tools per day": "https://www.zora.uzh.ch/id/eprint/136503/1/productiveWorkday_TSE17.pdf",
                ".NET": "https://devblogs.microsoft.com/dotnet/",
                "Visual Studio": "https://devblogs.microsoft.com/visualstudio/",
                "VS Code": "https://devblogs.microsoft.com/vscode-blog",
                "GitHub": "https://github.blog/",
                "Welcome to the Microsoft for Developers blog": "https://devblogs.microsoft.com/blog/welcome-to-the-microsoft-for-developers-blog",
                "Microsoft for Developers": "https://devblogs.microsoft.com"
            },
            "_rid": "JfZBAIxX5c4OAAAAAAAAAA==",
            "_self": "dbs/JfZBAA==/colls/JfZBAIxX5c4=/docs/JfZBAIxX5c4OAAAAAAAAAA==/",
            "_etag": "\"0000a5d3-0000-1500-0000-67e537a40000\"",
            "_attachments": "attachments/",
            "ai_summary": "Developers typically use an average of 16 tools daily, including code editors, command-line tools, and various SDKs, which can complicate integration and productivity. Microsoft for Developers aims to provide a unified platform to share insights and guidance on how to effectively use its products, such as Visual Studio and GitHub, while also focusing on AI integration and cloud migration. The new blog will serve as a central hub for major announcements, best practices, and community stories, while maintaining the existing ecosystem of trusted developer blogs.",
            "process_date": "2025-03-27T14:33:56.095835",
            "published": false,
            "publication_date": null,
            "_ts": 1743075236
        }
    ]
}